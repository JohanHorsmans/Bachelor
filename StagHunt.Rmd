---
title: "StagHunt"
author: "Kenneth Enevoldsen"
date: "9/19/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#make the competition into a vector operation
apply(pairs, 1, sum) #simple example
apply(pairs, 1, staghunt) #simple example


#Packages and WD
```{r Packages and WD}
setwd("/Users/kennethenevoldsen/Desktop/Bachelor/Bachelor_code/Bachelor/")
library(pacman)
p_load(plyr, tidyverse, raster, reshape2, knitr, brms, boot, rethinking)
```

#Game board
```{r  Game board}
#0: Dark space
#1: Walkable space
#2: Rabbit
#3: Stag
#4: Player 1 
#5: Player 2

  #creating the board
map_grid <- c(0, 0, 2, 0, 0, rep(1, 5), 1, 0, 1, 0, 1, rep(1,5), 0, 0, 2, 0, 0)
Stagmatrix <- matrix(data = map_grid, nrow = 5)

#simple plot
 # plot(raster(Stagmatrix))


#trying it out with players and stag
Stagmatrix[1,2] = 4
Stagmatrix[3,2] = 3
Stagmatrix[5,4] = 5

x1=melt(t(Stagmatrix))
names(x1)=c("x","y","color")

  
  #specifying the colors for the factors
x1$color=factor(x1$color)
levels(x1$color)=c("dark","walk", "rabbit", "stag", "player1", "player2")
colours = c("dark" = "black", "walk" = "white",  "rabbit" = "red4", "stag" = "red", "player1" = "green", "player2" = "green4")

  #plotting the board
qplot(x, y, fill=color, data=x1, geom='tile') + scale_fill_manual(values = colours)
```

#Payoff matrix
```{r Payoff matrix} 
choice_mapping <- data_frame(stag = c(1), rabbit = c(0))

pretty_p_matrix <- data_frame(x = c("stag", "rabbit"), stag = c("5,5", "3,0"), rabbit = c("0,3", "3,3"))
pretty_p_matrix

p_matrix <- data_frame(p1_choice = c(1, 0, 1, 0), #rm?
                       p2_choice = c(1, 1, 0, 0), 
                       p1_reward = c(5, 3, 0, 3), 
                       p2_reward = c(5, 0, 3, 3)) 
p_matrix
```

#Functions
```{r functions}
#contains a mixture of functions used for strategies

###_________________________ General _________________________###

#utility function
U <- function(a.self, a.op){
  # takes two arguments
  # a.self: choice of self
  # a.op: choice of opponnent
  # returns the point reward
  a.self*a.op*5 + (1-a.self)*3 #if both choose 1 you get 5 points and if you choose 0 you get 3 points
}

#softmax functions
softmax <- function(e_payoff_diff, beta){
  # Takes two arguments
    # expected payoff difference
    # a beta also called bahaivoural temperature - higher make the agent choice more random
  # Returns probability self choosing 1
  1/(1 + exp(-(e_payoff_diff/beta)))
}



###______________________ ToM specific _______________________###

#Probability of opponnent a^op choosing 1
prop_a.op1 <- function(mu, variance, sigma){
  # takes in three arguments
    # mu is approximate mean of 0-ToM posterior distribution
    # variance is subjective uncertainty of the mean 
    # sigma #?#
  # returns probability of opponent choosing 1 
  inv.logit(mu/sqrt(1+(variance+sigma)*3/pi^2))
}

mu_update <- function(mu, variance, a.op){
  # takes 3 arguments
    # mu from the round before
    # variance form the round before
    # a.op, which is the opponents choice last round
  # returns an updated mu
  mu + variance * (a.op - inv.logit(mu))
}

variance_update <- function(mu, variance, sigma) {
  # takes 3 arguments
    # mu from last round
    # variance from last round
    # sigma: higher values mean lower updating
  #returns an updated variance
  1 / (1 / (variance + sigma) + inv.logit(mu) * (1 - inv.logit(mu)))
}

```

#Strategies
```{r Strategies} 

###______________________ Simple agents ______________________###

#random choice
random_choice <- function(ID){
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences (personal biases)
    part_df$param[part_df$ID == ID] <-  list(c(prop = inv.logit(rnorm(1, mean = 0, sd = 0.1)))) #?# er den her fin? (skal der ikke være en SD?)
  }
  
  #randomly select rabbit or stag
  choice <- rbinom(1, 1, prob = part_df$param[part_df$ID == ID][[1]]['prop']) 
  return(choice)
}

#Win stay loose switch - should capture both nash, however this does not differentiate between reward
WSLS <- function(ID){ 
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences (personal biases)
    part_df$param[part_df$ID == ID] <-  list(c(noise = rnorm(1, mean = .10, sd = 0.1)))
  }
  
  current_pair <-  paste(pairs[pair,], collapse = "/")
  last_round <- result_df[result_df$pair == current_pair & result_df$round_nr == n_round-1,]

  if (empty(last_round)){ #if there was no last round then select randomly
    choice <- rbinom(1, 1, prob = 0.5)
  } else { #else WSLS
    choice <- last_round$choice[last_round$ID == ID]*last_round$choice[last_round$ID != ID]
  }
  
  #add noise
  noise <- part_df$param[part_df$ID == ID][[1]]['noise']
  choice <- rbinom(1, 1, prob = abs(choice - noise))
  
  return(choice)
}

#Tit for Tat - If you COOP I will COOP - this differentiate between reward and is more likely to priotize COOP if it given positive results
SoftmaxTitTat <- function(ID) { #!# not tested
  
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences (personal biases)
    part_df$param[part_df$ID == ID] <-  list(c(bavioral_temp = rnorm(1, mean = .5, sd = 0.2)))
  }
  
  beta <- part_df$param[part_df$ID == ID][[1]]['bavioral_temp']
  
  current_pair <-  paste(pairs[pair,], collapse = "/")
  last_round <- result_df[result_df$pair == current_pair & result_df$round_nr == n_round-1,]

  if (empty(last_round)) { #if there was no last round then select randomly based upon expected payoff
    e_payoff_diff <- payoff_difference(beta)
    choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta)) #!# the beta should change based on individual
  } else {
  e_payoff_diff <- payoff_difference(last_round$choice[last_round$ID != ID])
  choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  }
  
  return(choice)
}

###_______________Reinforcement learning agents_______________###

#Reinforcement learning #?#Should I use Monte Carlo Method or Q-learning? (se slides on reinforcemnt)

###________________________ ToM agents _______________________###
  
  #see Devaine, et al. (2017) https://doi.org/10.1371/journal.pcbi.1005833

#k=0
ToM0 <- function(a.op, beta, sigma, mu, variance){
  #update parameters based on previous choices 
  variance <- variance_update(mu, variance, sigma) #!# make this into an update variable (måske)
  mu <- mu_update(mu, variance, a.op)
  
  
  e_prop_a.op1 <- prop_a.op1(mu, variance, sigma) #estimated probability of opponent chosing 1 in prop
  e_payoff_diff <- payoff_difference(e_prop_a.op1) #estimated payoff difference
  prop_a.self1 <- softmax(e_payoff_diff, beta) #probability of self chosing 1
  
  return(choice)
}

#kig på hvordan du laver disse:
#1st
  #tjek her
#3rd

#5th


```

#Creating participants and setting number of rounds and participant
```{r Creating part and setting variables}
 #number of participants
n_p <- 10

  #percentage of strategies in participants
strategy_list <- c("random_choice", "WSLS1", "WSLS2")
strategy_prop <- c(1.0, 0.0, 0.0)

#number of round
n_rounds <- 10

#Matchup type
matchup_type = "random"

  #creating participant according to percantage
part_df <- data_frame(ID=seq(n_p), 
    strategy=sample(strategy_list, n_p, prob=strategy_prop, replace=TRUE),
    param = NA)
part_df$strategy <- as.character(part_df$strategy)

```

#Battle royale
```{r Battle royale}

#generating pairs to compete based on matchup type
if (matchup_type == "random"){ 
    #random matchup (participant is randomly matched up with another participant)
  pairs <- matrix(sample(part_df$ID, n_p, replace=F), ncol = 2) #pairs the participants randomly
} else if (matchup_type == "RR"){
    #round robin matchup (each participant battles all other participants)
  pairs <- t(combn(part_df$ID, 2)) #create all possible pairs
} else {
  print("ERROR: please chose a valid matchup_type")
}



apply(pairs, 1, staghunt)

#looping through the pairs making them compete for multiple rounds 
for (pair in 1:nrow(pairs)){
  print(pairs[pair,]) #!# should be removed but it is nice for now
    for (n_round in 1:n_rounds){
        #generate choice
      
      p1_choice <- do.call(part_df$strategy[part_df$ID == pairs[pair,1]], 
                           args = list(pairs[pair,1]))#calls the strategy the players uses (fetched in part_df)
      p2_choice <- do.call(part_df$strategy[part_df$ID == pairs[pair,2]], 
                           args = list(ID = pairs[pair,2]))
      
        #generate result
      result <- p_matrix[p_matrix$p1_choice == p1_choice & p_matrix$p2_choice == p2_choice,]
      
      #save results
      round_result <- data_frame(ID = pairs[pair,], 
                                 choice = c(p1_choice, p2_choice),
                                 points = c(result$p1_reward, result$p2_reward), 
                                 round_nr = n_round,
                                 strategy = c(part_df$strategy[part_df$ID == pairs[pair,1]],
                                              part_df$strategy[part_df$ID == pairs[pair,2]]),
                                 pair = paste(pairs[pair,], collapse = "/")
                                 )
      
      if (n_round == 1 & pair == 1){
        result_df <- round_result
      } else {
        result_df <- rbind(result_df, round_result)
      }
    }
}
```


#results
```{r Results}
#summing up the point for each partipant
sum_result <- result_df %>% 
  dplyr::group_by(ID, strategy) %>% 
  dplyr::summarise(total_points = sum(points))
sum_result




```


#Riccardo's code
```{r RF code} 
d<-d[complete.cases(d),]
d$Left=as.numeric(d$Left)
d$Handedness=1
d$Handedness[d$Left==0]=-1
d$Success=1
d$Success[d$IndividualPayoff==0]=0
d$Failure=0
d$Failure[d$IndividualPayoff==0]=1
d$X1=d$Handedness*d$Success
d$X2=d$Handedness*d$Failure
d$StayBias=c(NA,d$X1[1:(nrow(d)-1)])
d$LeaveBias=-c(NA,d$X2[1:(nrow(d)-1)])

WinStay <-
  brm(
    Left ~ 0 + StayBias + LeaveBias + (0 + StayBias + LeaveBias | Subject),
    d,
    family = bernoulli,
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.99, max_treedepth = 20),
    iter = 3e3
  )
```


