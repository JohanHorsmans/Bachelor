---
title: "KToM Function"
author: "Kenneth, Peter"
date: "18/11/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#To Do:

Bugfix smaller functions

Figure out the dummy. Should be in params. But figure out what it is

the VBA Elogsig - outputting only one value

Figure out the details of the param_mean indexing

Consider the "flag for noisy" - which parameter is affected by that ? Why is it a different one than the one being affected by volatility?


EXTRA
Handedness
Perturb Learning


#General functions
```{r}
#Utility function
U <- function(a_self, a_op, player, p_matrix){ 
  #INPUT
    #a_self: choice of self
    #a_op: choice of opponnent
    #player: which side of the payof matrix is used. 0: first player, 1: second player
    #returns the point reward
  #OUTPUT
    #The utility of of the given choices for the player.
  
  # get values from payoff matrix for player 1
  a_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # get values from payoff matrix for player 2
  a_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # calculate reward
  reward = 
    (1-player) * #for player 1
      (a_self * a_op * a_1 +           #if both choose 1 
       a_self * (1 - a_op) * b_1 +     #if you choose 1 and opponent chooses 0
       (1 - a_self) * a_op * c_1 +     #if you choose 0 and the opponent chooses 1
       (1 - a_self) * (1 - a_op) * d_1 #if both choose 0
       ) + 
    player * #for player 2
      (a_self * a_op * a_2 +           #if both choose 1 
       a_self * (1 - a_op) * b_2 +     #if you choose 1 and opponent chooses 0
       (1 - a_self) * a_op * c_2 +     #if you choose 0 and the opponent chooses 1
       (1 - a_self) * (1 - a_op) * d_2 #if both choose 0
       )
  
  return(reward)
}


#Expected payoff
expected_payoff_difference <- function(p_op_1, player, p_matrix) {
  
  e_payoff_dif = 
    p_op_1 * (U(1, 1, player, p_matrix) - U(0, 1, player, p_matrix)) +
    (1 - p_op_1) * (U(1, 0, player, p_matrix) - U(0, 0, player, p_matrix))
  
  return(e_payoff_dif)
}


#Softmax function
softmax <- function(e_payoff_diff, params){ 
  #Input
  behavioural_temperature = params$behavioural_temperature #the behavioural temperature mkaes the agent explore more
  
  #prepare behavioural temperature
  behavioural_temperature = exp(behavioural_temperature)
  
  #Calculate probability of choosing 1
  p_self_1 = 1 / (1 + exp(-(e_payoff_diff / behavioural_temperature)))
  
  return(p_self_1)
}
```

#Learning function - 0ToM estimate update
```{r}
basic_variance_update = function(prev_hidden_states, params) {
  #input
  volatility = params$volatility #the volatility parameter reduces learning, assuming that there is noise in the opponents decisions
  prev_variance_basic = prev_hidden_states$own_hidden_states$variance_basic #the uncertainty of opponent probability
  prev_mean_basic = prev_hidden_states$own_hidden_states$mean_basic #the mean estimate of opponent probability
  
  #prepare volatility
  volatility = exp(volatility)
  #prepare variance 
  prev_variance_basic = exp(prev_variance_basic)
  
  #calculate the new variance
  variance_basic = 
    1 / (
      (1 / (volatility + prev_variance_basic)) +
        inv.logit(prev_mean_basic) * (1 - inv.logit(prev_mean_basic)))
  
  #logistic transform
  variance_basic = log(variance_basic)
  
  return(variance_basic)
}

basic_mean_update = function (prev_hidden_states, choices, variance_basic) {
  #input
  prev_c_op = choices[2] #opponent's choice
  prev_mean_basic = prev_hidden_states$own_hidden_states$mean_basic #the uncertainty of opponent probability
  variance_basic #the uncertainty of opponent probability
  
  #prepare variance
  variance = exp(variance_basic)
  
  #calculate the new mean
  mean_basic = prev_mean_basic + variance_basic * (prev_c_op - inv.logit(prev_mean_basic))
  
  return(mean_basic)
}
```

#Learning function - update p(k)
```{r}
p_op_1_k_approx_fun <- function(prev_hidden_states){
  #this calculation uses a semi-analytical approximation by Daunizeau, J. (2017)
  #this is the VBA_Elogsig
  
  #input
  #for each sophistication level
  prev_mean = prev_hidden_states$own_hidden_states$mean # mean of opponent probability estimation VECTOR
  prev_variance = prev_hidden_states$own_hidden_states$variance # the variance for each estimated parameter MATRIX
  prev_gradient = prev_hidden_states$own_hidden_states$gradient # gradients for each parameter MATRIX
  
  #constants
  a <- 0.205
  b <- -0.319
  c <- 0.781
  d <- 0.870
  
  #prepare variance
  prev_variance = exp(prev_variance)*prev_gradient^2
  
  #calculate estimated probability of opponent choice
  p_op_1_k_approx = 
   inv.logit((prev_mean + b * prev_variance^c) / sqrt(1 + a * prev_variance^d))
 
  #log-transform
  p_op_1_k_approx = log(p_op_1_k_approx)
  
  return(p_op_1_k_approx)
}


update_pk <- function(prev_hidden_states, choices, p_op_1_k_approx){
  #input
  prev_c_op = choices[2] #opponent's choice
  #for each sophistication level
  prev_p_k = prev_hidden_states$own_hidden_states$p_k  # probability of sophistication level k VECTOR
  p_op_1_k_approx # probability of opponent choosing 1, approximated semi-analytically VECTOR
  
  #prepare probability
  p_op_1_k_approx = exp(p_op_1_k_approx)
  
  #calculate probability of each possible sophistication level
  p_k =
    prev_c_op* #if opponent chose 1
    ((prev_p_k*p_op_1_k_approx)/sum(prev_p_k*p_op_1_k_approx)) + 
    (1-prev_c_op)* #if opponent chose 0
    (prev_p_k*(1-p_op_1_k_approx)/sum(prev_p_k*(1-p_op_1_k_approx))) 
  
  return(p_k)
}

```

#Learning function - updating parameter estimates
```{r}
parameter_variance_update <- function(prev_hidden_states, params, p_k) {
  #input
  volatility = params$volatility
  #for each k:
  prev_param_mean = prev_hidden_states$own_hidden_states$param_mean #the mean for each estimated parameter MATRIX
  prev_variance = prev_hidden_states$own_hidden_states$variance #the uncertainty for each estimated parameter MATRIX
  prev_gradient = prev_hidden_states$own_hidden_states$gradient #the gradient for each estimated parameter MATRIX
  p_k #the probability of sopistication level k VECTOR
  
  #prepare volatility
  dummy = c(1,0,0) #dummy parable flags which parameters are affected by volatility #?# We need to figure out which parameter this is targeting!
  volatility = exp(volatility)*dummy
  
  #prepare variance
  prev_variance = exp(prev_variance)
  
  #calculate new variance
  variance = 
  1 / 
    (1 / (prev_variance + volatility) + 
       p_k * 
       inv.logit(prev_param_mean) * (1 - inv.logit(prev_param_mean)) * 
       prev_gradient^2)
  
  #logistic transform
  variance = log(variance)
  
  return(variance)
}  


parameter_mean_update <- function(prev_hidden_states, choices, p_k, variance){
  #input
  prev_c_op = choices[2] #opponent's choice
  #for each sophistication level k:
  prev_mean = prev_hidden_states$own_hidden_states$mean #the mean of opponent probability estimation VECTOR
  prev_param_mean = prev_hidden_states$own_hidden_states$param_mean #the mean for each estimated parameter MATRIX
  p_k #the probability of sophistication level k VECTOR
  variance #the variance of each estimated parameter MATRIX
  
  #prepare variance
  variance = exp(variance)*prev_gradient
  
  #calculate new mean estimates
  param_mean = 
    prev_param_mean + p_k * variance * (prev_c_op - inv.logit(prev_mean))
  
  #?# "for numerical purposes" - unsure if necessary
  #param_mean = inv.logit(logit(param_mean))
  
  return(param_mean)
}

```

#Learning function - updating gradient
```{r}
gradient_update = function(opponent_prev_hidden_states, mean, param_mean, reverse_choices, opponent_level, opponent_player, p_matrix) {
  #input
  opponent_prev_hidden_states #opponent's hidden states, for running the learning function
  reverse_choices #opponent's perspective
  opponent_level #
  opponent_player #
  mean #the mean of opponent probability estimation VECTOR
  param_mean #the mean for each estimated parameter MATRIX
  
  #increment parameters
  increment = max(abs(1e-4*param_mean), 1e-4)
  param_mean_incremented = param_mean + increment
  
  for (param in length(param_mean)) {
      #use the parameter estimates
      param_mean_temp = param_mean 
      #but use one of the incremented instead
      param_mean_temp[param] = param_mean_incremented[param] 
      
      #Make a list for parameters to be inserted
      opponent_params = list(
      volatility = param_mean_temp[1], #volatility is the first column in the matrix
      behavioural_temperature = param_mean_temp[2] #temperature is the second column in the matrix
      ) 
      
      #run the learning function of opponent using param_mean_temp as parameter values
      opponent_hidden_states_incremented = rec_learning_function(prev_hidden_states = opponent_prev_hidden_states, 
                                                           params = param_mean_temp, 
                                                           choices = reverse_choices,
                                                           level = opponent_level,
                                                           player = opponent_player) 
      
      #run the decision function of opponent using the temporary hidden states
      mean_incremented = decision_function(hidden_states = opponent_hidden_states_incremented,
                                           params = params_mean_temp,
                                           player = opponent_player,
                                           p_matrix = p_matrix)
      
      #calculate the gradient between parameter increment and probability estimate
      gradient[param] = (mean_incremented - mean)/increment[param]
  }
  
  return(gradient)
}
```

#Full learning function
```{r}
rec_learning_function = function(
  prev_hidden_states,
  params,
  choices,
  level,
  player,
  p_matrix
) {
  #Make empty list for filling with updated values
  new_hidden_states = list()
  
  if (level == 0) { #If the (simulated) agent is a 0-ToM
    
    #Update 0-ToM's uncertainty of opponent choice probability
    variance_basic = basic_variance_update(prev_hidden_states, params)
    #Update 0-ToM's mean estimate of opponent choice probability
    mean_basic = basic_mean_update(prev_hidden_states, choices, variance_basic)
    
    #Gather own hidden states into one list
    own_hidden_states = list(mean_basic = mean_basic, variance_basic = variance_basic)
    
  } else { #If the (simulated) agent is a K-ToM
  
    #Update p_k
    p_op_1_k_approx = p_op_1_k_approx_fun(prev_hidden_states)
    p_k = update_pk(prev_hidden_states, choices, p_op_1_k_approx)
    
    #Update parameter estimates
    variance = parameter_variance_update(prev_hidden_states, params, p_k)
    param_mean = parameter_mean_update(prev_hidden_states, choices, p_k, variance)
    
    #Prepare opponent's perspective
    reverse_choices = choices[2:1]
    opponent_player = 1-player
    
    #Now we need to go through each possible opponent's level one at a time. Highest opponent level is 1 lower than own level
    for (level_index in 1:level) {
      
      #Set the simulated opponents level. "level_index" is one higher than the actual level because it isn't possible to index 0
      opponent_level = level_index-1
      #Extract the currently simulated opponent's hidden states
      opponent_hidden_states = prev_hidden_states[[level_index]]
      #Extract the estimated parameters of the current opponent
      opponent_params = list(
        volatility = param_mean[level_index, 1], #volatility is the first column in the matrix
        behavioural_temperature = param_mean[level_index, 2] #temperature is the second column in the matrix
        ) 
      
      #Simulate opponent learning
      new_opponent_hidden_states = rec_learning_function(prev_hidden_states = opponent_hidden_states,
                                                     params = opponent_params,
                                                     choices = reverse_choices,
                                                     level = opponent_level,
                                                     player = opponent_player,
                                                     p_matrix = p_matrix) 
      
      #Simulate opponent deciding
      mean[level_index] = decision_function(hidden_states = new_opponent_hidden_states,
                                            params = opponent_params,
                                            player = opponent_player,
                                            p_matrix = p_matrix)
      
      #Update gradient
      gradient[level_index,] = gradient_update(opponent_prev_hidden_states = opponent_hidden_states,
                                 mean = mean[level_index],
                                 param_mean = param_mean[level_index,], #only input the param_mean for the current level
                                 reverse_choices = reverse_choices,
                                 opponent_level = opponent_level,
                                 opponent_player = opponent_player,
                                 p_matrix = p_matrix)
      
      #Save opponent's new hidden states
      new_hidden_states[[level_index]] = new_opponent_hidden_states
    }
    
    #Gather own hidden states into one list
    own_hidden_states = list(p_k = p_k, mean = mean, param_mean = param_mean, variance = variance, gradient = gradient)
  }
  
  #Save own updated hidden states to new hidden states
  new_hidden_states$own_hidden_states = own_hidden_states
  
  return(new_hidden_states)
}
```

#Decision function - 0-ToM opponent probability
```{r}
#Probability of opponnent a_op choosing 1
basic_p_op_1_fun <- function(hidden_states){

  #THIS IS THE VARIANT USED IN THE MATLAB CODE, WHICH DOES NOT USE VOLATILITY
  
  #for each sophistication level k:
  mean_basic = hidden_states$own_hidden_states$mean_basic #mean opponent probability estimate VECTOR
  variance_basic = hidden_states$own_hidden_states$variance_basic #variance of parameter estimates MATRIX
  a = 0.36 #this number is taken from the matlab code in ObsRecGen
  
  #Prepare variance
  variance_basic = exp(variance_basic)
  
  #calculate opponent's probability of choosing 1
  p_op_1_basic = inv.logit(mean_basic / sqrt(1 + a * variance_basic)) 
  
  return(p_op_1_basic)
}

###ALTERNATIVE VERSION
# basic_p_op_1_fun <- function(own_hidden_states, params){
# 
#   #THIS IS THE VARIANT BASED ON THE EQUATIONS, BUT NOT USED IN THE MATLAB CODE
# 
#   #input
#   volatility = params$volatility
#   mean_basic = own_hidden_states$mean_basic #mean opponent probability estimate VECTOR
#   variance_basic = own_hidden_states$variance_basic #variance of parameter estimates MATRIX
#   
#   #Prepare variance
#   variance_basic = exp(variance_basic)
#   
#   #prepare volatility
#   volatility = exp(volatility)
#   
#   #calculate opponent's probability of choosing 1
#   p_op_1_basic = inv.logit(mean_basic/sqrt(1+(variance_basic+volatility)*3/pi^2))
#   
#   return(p_op_1_basic)
# }

```

#Decision function - opponent probability
```{r}
#Probability of opponnent a_op choosing 1
p_op_1_k_fun <- function(hidden_states){

  #THIS IS THE VARIANT USED IN THE MATLAB CODE, WHICH DOES NOT USE VOLATILITY
  
  #for each sophistication level k:
  mean = hidden_states$own_hidden_states$mean #mean opponent probability estimate VECTOR
  variance = hidden_states$own_hidden_states$variance #variance of parameter estimates MATRIX
  a = 0.36 #this number is taken from the matlab code in ObsRecGen
  
  #Prepare variance
  variance = rowSums(exp(variance) * gradient^2) #summing the variances of each parameter (after weighting by gradient). One sum per sophistication level
  
  #calculate opponent's probability of choosing 1
  p_op_1_k = inv.logit(mean / sqrt(1 + a * variance)) 
  
  return(p_op_1_k)
}

###ALTERNATIVE VERSION
# p_op_1_fun_k <- function(mean, variance, params){
# 
#   #THIS IS THE VARIANT BASED ON THE EQUATIONS, BUT NOT USED IN THE MATLAB CODE
#   
#   #input
#   volatility = params$volatility
#   #for each sophistication level k:
#   mean #mean opponent probability estimate VECTOR
#   variance #variance of parameter estimates MATRIX
#   
#   #prepare volatility
#   dummy = c(1,0,0) #dummy parable flags which parameters are affected by volatility #?# We need to figure out which parameter this is targeting!
#   volatility = exp(volatility)*dummy
#   
#   #Prepare variance
#   variance = sum(exp(variance)*gradient^2)
#   
#   #calculate opponent's probability of choosing 1
#   p_op_1_k = inv.logit(mean/sqrt(1+(variance+volatility)*3/pi^2)) 
#   
#   return(p_op_1_k)
# }

```

#Full decision function 
```{r}
decision_function = function(
  hidden_states,
  params,
  player,
  p_matrix
) { 
  
  if (level == 0) { #If the (simulated) agent is a 0-ToM
    
    #Calculate opponent probability of choosing 1
    p_op_1 = basic_p_op_1_fun(hidden_states)
    
  } else { #If the (simulated) agent is a K-ToM
    
    #Calculate opponent probability of choosing 1, for each k
    p_op_1_k = p_op_1_k_fun(hidden_states)
  
    #extract probabilities for each opponent level
    p_k = hidden_states$own_hidden_states$p_k
    
    #Weigh probabilities by corresponding level probabilities, to calculate an aggregate probability of opponent choosing 1
    p_op_1 = sum(p_op_1_k * p_k)
  }
  
  #Calculate the expected payoff difference
  e_payoff_dif = expected_payoff_difference(p_op_1, player, p_matrix)
  
  #Put into the softmax function to calculate the probability of choosing 1
  p_self_1 = softmax(e_payoff_dif, params)
  
  return(p_self_1)
  }

```

#Full k-ToM function
```{r}
k_ToM = function (params, hidden_states, player, level, p_matrix, choice_self, choice_op, return_hidden_states = T) {
  
  #the input comes from last round
  prev_hidden_states = hidden_states
  #bind choices together for easy reorganising
  choices = c(choice_self, choice_op)
  
  if (is.null(choice_self)){ #If first round or missed trial
    
    new_hidden_states = prev_hidden_states #No update
    
  } else {
    
    #Update hidden states
     new_hidden_states = 
      rec_learning_function(
        prev_hidden_states,
        params,
        choices,
        level,
        player,
        p_matrix)
  }
  
  #Calculate decision probability
  p_self_1 = 
    decision_function(
      hidden_states = new_hidden_states,
      params,
      player,
      p_matrix) 
  
  #Choose
  choice = rbinom(1, 1, p_self_1)
  
  return(choice, new_hidden_states)
}
```

#Preparing hidden states for the k-ToM function
```{r}
rec_prepare_k_ToM = function(level, priors = "default") {
  
  if (class(priors) != "list"){ #Use default priors if nothing else is specified
    
    message("No priors specified, using default priors")
    
    priors <- list(mean_basic = 0, #agnostic
                   variance_basic = 0, #will be exponated, so exp(0) = 1
                   mean = 0, #agnostic
                   variance = c(0,0), #will be exponated, so exp(0) = 1
                   param_mean = c(0,0), #agnostic
                   gradient = c(0,0) #is zero. in the Matlab script, one parameter was set to 1 if "flag for noisy" was set
                   )
  }
  
  #Make empty list for filling with updated values
  new_hidden_states = NULL
  
  if (level == 0) { #If the (simulated) agent is a 0-ToM
    
    #Set prior variance on 0-ToM's estimate of opponent choice probability
    variance_basic = priors$variance_basic
    #Set prior mean on 0-ToM's estimate of opponent choice probability
    mean_basic = priors$mean_basic
    
    #Gather own hidden states into one list
    own_hidden_states = list(mean_basic = mean_basic, variance_basic = variance_basic)
    
  } else { #If the (simulated) agent is a K-ToM
    
    #Set priors, one for each of opponent's possible sophistication level
    #Probability is agnostic
    p_k = rep(1, level)/level
    #Mean of opponent choice probability estimate
    mean = rep(priors$mean, level)
    #Variance on parameter estimates
    variance = t(matrix(rep(priors$variance, level),
                        nrow = length(priors$variance)))
    #Mean of parameter estimates
    param_mean = t(matrix(rep(priors$param_mean, level),
                          nrow = length(priors$param_mean)))
    #Gradient
    gradient = t(matrix(rep(priors$gradient, level),
                      nrow = length(priors$gradient)))
    
    #Gather own hidden states into one list
    own_hidden_states = list(p_k = p_k, mean = mean, param_mean = param_mean, variance = variance, gradient = gradient)
      
    #Now we need to go through each possible opponent's level one at a time. Highest opponent level is 1 lower than own level
    for (level_index in 1:level) {
      
      #Set the simulated opponents level. "level_index" is one higher than the actual level because it isn't possible to index 0
      opponent_level = level_index-1
      
      #Get hidden states from simulated opponents
      new_hidden_states[[level_index]] = rec_prepare_k_ToM(level = opponent_level,
                                                           priors) 
    }
  }
  
  #Save own updated hidden states to new hidden states
  new_hidden_states$own_hidden_states = own_hidden_states
  
  return(new_hidden_states)
}

```

#Testing Space
```{r}
test_hidden_states = rec_prepare_k_ToM(level = 3)

penny_competitive_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 1, 1, 0), 
             a2_reward = c(1, 0, 0, 1))

params_test = list(behavioural_temperature = -1,
              volatility = -2)

k_ToM (params = params_test,
       hidden_states = test_hidden_states,
       player = 1,
       level = 3,
       p_matrix = penny_competitive_matrix,
       choice_self = 0,
       choice_op = 1)

```

