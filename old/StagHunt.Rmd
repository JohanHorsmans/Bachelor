---
title: "StagHunt"
author: "Kenneth Enevoldsen"
date: "9/19/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

To do:
do a simulation of good sigma values (see RF simulation code)
add equation number
make an always coop and never coop 


To do optionally:
Make a prepare agents function (prepares all the variables for the agent in the beginning)
make the competition into a vector operation
  apply(pairs, 1, sum) #simple example
  apply(pairs, 1, staghunt_compete) 
make it into a single function
make more reasonable variable names (eg. a.op -> choice_op eller c.op eller måske c1.op)
part_df should be agents_df
make some kind of extended DF for all states of the participants

 
General outline:
-create desired agents (DONE)
-prepare agents (create initial variable, biases etc.) (make a function which prepares based on strategy)
-pair up agents (DONE)
-make agents compete (make a staghunt_compete function, and make it work with apply)


#Packages and WD
```{r Packages and WD}
setwd("/Users/kennethenevoldsen/Desktop/Bachelor/Bachelor_code/Bachelor/")
library(pacman)
p_load(plyr, tidyverse, raster, reshape2, knitr, brms, boot, rethinking)
```

#Example test data #!# temporary
```{r Example data}
#data for intended for testing
test_df <- result_df

```

#Game board
```{r  Game board}
#0: Dark space
#1: Walkable space
#2: Rabbit
#3: Stag
#4: Player 1 
#5: Player 2

  #creating the board
map_grid <- c(0, 0, 2, 0, 0, rep(1, 5), 1, 0, 1, 0, 1, rep(1,5), 0, 0, 2, 0, 0)
Stagmatrix <- matrix(data = map_grid, nrow = 5)

#simple plot
 # plot(raster(Stagmatrix))


#trying it out with players and stag
Stagmatrix[1,2] = 4
Stagmatrix[3,2] = 3
Stagmatrix[5,4] = 5

x1=melt(t(Stagmatrix))
names(x1)=c("x","y","color")

  
  #specifying the colors for the factors
x1$color=factor(x1$color)
levels(x1$color)=c("dark","walk", "rabbit", "stag", "player1", "player2")
colours = c("dark" = "black", "walk" = "white",  "rabbit" = "red4", "stag" = "red", "player1" = "green", "player2" = "green4")

  #plotting the board
qplot(x, y, fill=color, data=x1, geom='tile') + scale_fill_manual(values = colours)
```

#Payoff matrix
```{r Payoff matrix} 
choice_mapping <- data_frame(stag = c(1), rabbit = c(0))

pretty_p_matrix <- data_frame(x = c("stag", "rabbit"), stag = c("5,5", "3,0"), rabbit = c("0,3", "3,3"))
pretty_p_matrix

p_matrix <- data_frame(p1_choice = c(1, 0, 1, 0), #rm?
                       p2_choice = c(1, 1, 0, 0), 
                       p1_reward = c(5, 3, 0, 3), 
                       p2_reward = c(5, 0, 3, 3)) 
p_matrix
```

#Functions
```{r functions}
#contains a mixture of functions used for strategies

###_________________________ General _________________________###

#utility function
U <- function(a.self, a.op){ 
  # takes two arguments
  # a.self: choice of self
  # a.op: choice of opponnent
  # returns the point reward
  a.self*a.op*5 + (1-a.self)*3 #if both choose 1 you get 5 points and if you choose 0 you get 3 points
}

#Expected payoff
payoff_difference <- function(prop_a.op1){
  prop_a.op1*(U(1,1)-U(0,1))+(1-prop_a.op1)*(U(1,0)-U(0,0))
}

#softmax functions
softmax <- function(e_payoff_diff, beta){ 
  # Takes two arguments
    # expected payoff difference
    # a beta also called bahaivoural temperature - higher make the agent choice more random
  # Returns probability self choosing 1
  1/(1 + exp(-(e_payoff_diff/beta)))
}



###______________________ ToM specific _______________________###

#Probability of opponnent a^op choosing 1
prop_a.op1 <- function(mu, variance, sigma){
  # takes in three arguments
    # mu is approximate mean of 0-ToM posterior distribution
    # variance is subjective uncertainty of the mean 
    # sigma #?#
  # returns probability of opponent choosing 1 
  inv.logit(mu/sqrt(1+(variance+sigma)*3/pi^2))
}

mu_update <- function(mu, variance, a.op){
  # takes 3 arguments
    # mu from the round before
    # variance form the round before
    # a.op, which is the opponents choice last round
  # returns an updated mu
  mu + variance * (a.op - inv.logit(mu))
}

variance_update <- function(mu, variance, sigma) {
  # takes 3 arguments
    # mu from last round
    # variance from last round
    # sigma: higher values mean lower updating
  #returns an updated variance
  1 / (1 / (variance + sigma) + inv.logit(mu) * (1 - inv.logit(mu)))
}

```

#Strategies (old, but working)
```{r Strategies}

###______________________ Simple agents ______________________###

#random choice
random_choice <- function(ID){ #!# not tested
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences (personal biases)
    part_df$param[part_df$ID == ID] <-  list(c(prop = inv.logit(rnorm(1, mean = logit(0.5), sd = 0.1)))) #?# er den her fin? (skal der ikke være en SD?)
  }

  #randomly select rabbit or stag
  choice <- rbinom(1, 1, prob = part_df$param[part_df$ID == ID][[1]]['prop']) 
  return(choice)
}

#Win stay loose switch - should capture both nash, however this does not differentiate between reward
WSLS <- function(ID){  
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences (personal biases)
    part_df$param[part_df$ID == ID] <-  list(c(noise = 
                                                 inv.logit(rnorm(1, mean = logit(0.1), sd = 0.1))))
  }
  
  current_pair <-  paste(pairs[pair,], collapse = "/")
  last_round <- try(result_df[result_df$pair == current_pair & result_df$round_nr == n_round-1,], 
                    silent = T)
  
  if (class(last_round)[1] == "try-error" | empty(last_round)){ #if there was no last round then select randomly
    choice <- rbinom(1, 1, prob = 0.5)
  } else { #else WSLS
    choice <- last_round$choice[last_round$ID == ID]*last_round$choice[last_round$ID != ID]
  }
  
  #add noise
  noise <- part_df$param[part_df$ID == ID][[1]]['noise']
  choice <- rbinom(1, 1, prob = abs(choice - noise))
  
  return(choice)
}

#Tit for Tat - If you COOP I will COOP - this differentiate between reward and is more likely to priotize COOP if it given positive results
#?# does this one make sense?
SoftmaxTitTat <- function(ID) { #!# not tested 
  
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences (personal biases)
    part_df$param[part_df$ID == ID] <-  list(c(bavioral_temp = rnorm(1, mean = .5, sd = 0.2)))
  }
  
  beta <- part_df$param[part_df$ID == ID][[1]]['bavioral_temp']
  
  current_pair <- paste(pairs[pair,], collapse = "/")
  last_round <- try(result_df[result_df$pair == current_pair & result_df$round_nr == n_round-1,], 
                    silent = T)
  
  if (class(last_round)[1] == "try-error" | empty(last_round)) { #if there was no last round then select randomly based upon expected payoff
    e_payoff_diff <- payoff_difference(beta)
    choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta)) #!# the beta should change based on individual
  } else {
  e_payoff_diff <- payoff_difference(last_round$choice[last_round$ID != ID])
  choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  }
  
  return(choice)
}

###_______________Reinforcement learning agents_______________###

#Reinforcement learning #?# Should I use Monte Carlo Method or Q-learning? (se slides on reinforcemnt)

###________________________ ToM agents _______________________###

  #see Devaine, et al. (2017) https://doi.org/10.1371/journal.pcbi.1005833

#k=0
ToM0 <- function(ID){ 
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences / starting point
    part_df$param[part_df$ID == ID] <- list(c(bavioral_temp = rnorm(1, mean = .5, sd = 0.2), 
                                              variance = rnorm(1, mean = 1.5, sd = 0.3),
                                              mu = rnorm(1, mean = 0.5, sd = 0.2), #this is in logodds #!# set it up so people are slightly more likely to COOP
                                              sigma = inv.logit(rnorm(1, mean = logit(0.3), sd = 0.2)) 
                                              ))

  }
  #fetching info from the participant with the corresponding ID
  beta <- part_df$param[part_df$ID == ID][[1]]['bavioral_temp']
  variance <- part_df$param[part_df$ID == ID][[1]]['variance']
  mu <- part_df$param[part_df$ID == ID][[1]]['mu']
  sigma <- part_df$param[part_df$ID == ID][[1]]['sigma'] 
  
  #getting info from the last round
  current_pair <- paste(pairs[pair,], collapse = "/")
  last_round <- try(result_df[result_df$pair == current_pair & result_df$round_nr == n_round-1,], 
                    silent = T) 
  
  if ((class(last_round)[1] == "try-error" | empty(last_round)) == FALSE){ #if the data isn't empty and there was no error reading it (e.g. if it was the very first round) - update the variable (this will almost always happen with the exception being the start of a round)
    
    a.op <- last_round$choice[last_round$ID != ID] #opponent last choice
  
    #update parameters based on opponnents previous choices 
    variance <- variance_update(mu, variance, sigma) #?# peter: var det variance der skulle være først?
    mu <- mu_update(mu, variance, a.op)
    
    #save new values of variance and mu
    part_df$param[part_df$ID == ID][[1]]['variance'] <- variance
    part_df$param[part_df$ID == ID][[1]]['mu'] <- mu
  }
  
  
  #calculate response
  e_prop_a.op1 <- prop_a.op1(mu, variance, sigma) #estimated probability of opponent chosing 1 in prop
  e_payoff_diff <- payoff_difference(e_prop_a.op1) #estimated payoff difference
  prop_a.self1 <- softmax(e_payoff_diff, beta) #probability of self chosing 1
  prop_a.self1
  
  choice <- rbinom(1, 1, prop_a.self1)
  
  
  
  return(choice)
}

```

#prepare agents #!# WIP
```{r}
#here I will create the prepare agents function
```



#Creating participants and setting number of rounds and participant
```{r Creating part and setting variables} 
 #number of participants
n_p <- 10

  #percentage of strategies in participants
strategy_list <- c("random_choice", "WSLS", "SoftmaxTitTat", "ToM0")
strategy_prop <- c(0.0, 0.0, 1.0, 0.0)

#number of round
n_rounds <- 30

#Matchup type
matchup_type = "random"

  #creating participant according to percantage
part_df <- data_frame(ID=seq(n_p), 
    strategy=sample(strategy_list, n_p, prob=strategy_prop, replace=TRUE),
    param = NA)
part_df$strategy <- as.character(part_df$strategy)

```


#the apply stuff - not working yet
```{r} 
#define a function for the competion
staghunt <- function(pair, n_rounds){
  #takes two arguments, a matrix of pairs and a number of rounds

  for (n_round in 1:n_rounds){
        #generate choice
      p1_choice <- do.call(part_df$strategy[part_df$ID == pairs[pair,1]],
                           args = list(pairs[pair,1]))#calls the strategy the players uses (fetched in part_df)
      p2_choice <- do.call(part_df$strategy[part_df$ID == pairs[pair,2]],
                           args = list(ID = pairs[pair,2]))

        #generate result
      result <- p_matrix[p_matrix$p1_choice == p1_choice & p_matrix$p2_choice == p2_choice,]

      #save results
      round_result <- data_frame(ID = pairs[pair,],
                                 choice = c(p1_choice, p2_choice),
                                 points = c(result$p1_reward, result$p2_reward),
                                 round_nr = n_round,
                                 strategy = c(part_df$strategy[part_df$ID == pairs[pair,1]],
                                              part_df$strategy[part_df$ID == pairs[pair,2]]),
                                 pair = paste(pairs[pair,], collapse = "/")
                                 )

      if (n_round == 1 & pair == 1){
        result_df <- round_result
      } else {
        result_df <- rbind(result_df, round_result)
      }
    }

}
apply(pairs, 1, function(x) print(paste(x, collapse = "/")))
```


#Battle royale
```{r Battle royale}

#generating pairs to compete based on matchup type
if (matchup_type == "random"){ 
    #random matchup (participant is randomly matched up with another participant)
  pairs <- matrix(sample(part_df$ID, n_p, replace=F), ncol = 2) #pairs the participants randomly
} else if (matchup_type == "RR"){
    #round robin matchup (each participant battles all other participants)
  pairs <- t(combn(part_df$ID, 2)) #create all possible pairs
} else {
  print("ERROR: please chose a valid matchup_type")
}


#reset result_df
try(rm(result_df), silent = T)


#looping through the pairs making them compete for multiple rounds 
for (pair in 1:nrow(pairs)){
  print(pairs[pair,]) #!# should be removed but it is nice for now
    for (n_round in 1:n_rounds){
        #generate choice
      
      p1_choice <- do.call(part_df$strategy[part_df$ID == pairs[pair,1]], 
                           args = list(pairs[pair,1]))#calls the strategy the players uses (fetched in part_df)
      p2_choice <- do.call(part_df$strategy[part_df$ID == pairs[pair,2]], 
                           args = list(ID = pairs[pair,2]))
      
        #generate result
      result <- p_matrix[p_matrix$p1_choice == p1_choice & p_matrix$p2_choice == p2_choice,]
      
      #save results
      round_result <- data_frame(ID = pairs[pair,], 
                                 choice = c(p1_choice, p2_choice),
                                 points = c(result$p1_reward, result$p2_reward), 
                                 round_nr = n_round,
                                 strategy = c(part_df$strategy[part_df$ID == pairs[pair,1]],
                                              part_df$strategy[part_df$ID == pairs[pair,2]]),
                                 pair = paste(pairs[pair,], collapse = "/")
                                 )
      
      if (n_round == 1 & pair == 1){
        result_df <- round_result
      } else {
        result_df <- rbind(result_df, round_result)
      }
    }
}
```


#results
```{r Results}
#summing up the point for each partipant
sum_result <- result_df %>% 
  dplyr::group_by(ID, strategy) %>% 
  dplyr::summarise(total_points = sum(points))
sum_result




```


#Riccardo's code
```{r RF code} 
d<-d[complete.cases(d),]
d$Left=as.numeric(d$Left)
d$Handedness=1
d$Handedness[d$Left==0]=-1
d$Success=1
d$Success[d$IndividualPayoff==0]=0
d$Failure=0
d$Failure[d$IndividualPayoff==0]=1
d$X1=d$Handedness*d$Success
d$X2=d$Handedness*d$Failure
d$StayBias=c(NA,d$X1[1:(nrow(d)-1)])
d$LeaveBias=-c(NA,d$X2[1:(nrow(d)-1)])

WinStay <-
  brm(
    Left ~ 0 + StayBias + LeaveBias + (0 + StayBias + LeaveBias | Subject),
    d,
    family = bernoulli,
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.99, max_treedepth = 20),
    iter = 3e3
  )



#Simulating values for beta (behavioral temperature)
x=data.frame(V=rep(NA,100),B=rep(NA,100),p=rep(NA,100))
n=1
for (V in seq(from=0, to=1, by=0.1)){
  for (B in seq(from=0, to=3, by=0.5)){
    print(n)
    x$B[n]=B
    x$V[n]=V
    x$p[n]=1 / (1+exp(-V/B))
    n=n+1
  }
}
ggplot(x, aes(V,p,group=B,color=B))+geom_smooth()


```


