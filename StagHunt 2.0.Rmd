---
title: "StagHunt 2.0"
author: "Kenneth C. Enevoldsen og Peter T. Waade"
date: "10/29/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

To do:
  do a simulation of good sigma values (see RF simulation code)
  (add equation number)
  for the 0-ToM there might be a double up on some of the code from k-ToM - make sure this is not the case
  
  Add "p_matrix"" and "player" to all functions. Currently done until and including WSLS.
  Figure out the relation between "ID" and "player". Only one of them should be necessary. Perhaps we can just use "player" to select column in the last_round and similar places? 
  Test the code. Especially: make it run in a competitive game. See if the higher k-levels usually win against the lower ones. Also test 0-ToM against RB's
  Edit the description for RB
  Edit the description of the U function


Minor issue:
  currently the script uses double lists for inputting lists into dataframe - this issue might be fixable using df[[i]], however I am not sure how this will affect the apply functions.
  The round robin does really work with the current setup for k-ToM as parameters in reset

#Packages and WD
```{r packages and wd} 
setwd("/Users/kennethenevoldsen/Desktop/Bachelor/Bachelor_code/Bachelor/")
library(pacman)
p_load(plyr, tidyverse, raster, reshape2, knitr, brms, boot, rethinking)
```

#Game board
```{r  game board} 
#0: Dark space
#1: Walkable space
#2: Rabbit
#3: Stag
#4: Player 1 
#5: Player 2

  #creating the board
map_grid <- c(0, 0, 2, 0, 0, rep(1, 5), 1, 0, 1, 0, 1, rep(1,5), 0, 0, 2, 0, 0)
Stagmatrix <- matrix(data = map_grid, nrow = 5)

#simple plot
 # plot(raster(Stagmatrix))


#trying it out with players and stag
Stagmatrix[1,2] = 4
Stagmatrix[3,2] = 3
Stagmatrix[5,4] = 5

x1=melt(t(Stagmatrix))
names(x1)=c("x","y","color")

  
  #specifying the colors for the factors
x1$color=factor(x1$color)
levels(x1$color)=c("dark","walk", "rabbit", "stag", "player1", "player2")
colours = c("dark" = "black", "walk" = "white",  "rabbit" = "red4", "stag" = "red", "player1" = "green", "player2" = "green4")

  #plotting the board
qplot(x, y, fill=color, data=x1, geom='tile') + scale_fill_manual(values = colours)
```

#Payoff matrix
```{r Payoff matrix}  
### Pretty Matrix ###
choice_mapping <- data_frame(stag = c(1), rabbit = c(0))

pretty_p_matrix <- data_frame(x = c("stag", "rabbit"), stag = c("5,5", "3,0"), rabbit = c("0,3", "3,3"))
pretty_p_matrix

### Payoff matrices for different games ###

custom_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 0, 0, 0), 
             a2_reward = c(0, 0, 0, 0))

staghunt_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(5, 3, 0, 3), 
             a2_reward = c(5, 0, 3, 3))

penny_cooperative_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(1, 0, 0, 1), 
             a2_reward = c(1, 0, 0, 1))

penny_competitive_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 1, 1, 0), 
             a2_reward = c(1, 0, 0, 1))

prisoner_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(2, 3, -1, 0), 
             a2_reward = c(2, -1, 3, 0))
```

#create_agents and match_up functions
```{r create_agents and match_up} 
create_agents <- function(n_agents = 10, strategy, prop){ 
  #INPUT
    #n_agents: the desired number of agents
    #n_rounds: the desired number of round
    #strategy: a list of strategies given as a character vector
      #implemented agent strategies
          #NCB: Never cooperate (with a bias)
          #CB: Always cooperate (with bias)
          #RB: Random Bias, random choice with a bias
          #WSLS: Win Stay Loose Switch (maybe not that relevant in this task?)
          #SoftmaxTitTat: A tit for tat using a softmax function as well as an utility function
          #k-ToM: a k-level theory of mind agent (e.g. k=0 => 0-ToM) 
            #Note that you input the desired k-level in place of k, e.g. 1-ToM
    #prop: a list of probabilities for the strategies (ordered)
  #OUTPUT
    #A dataframe of agents with the given strategies according to prop as well of the function which their strategies applies
  
  output <- data_frame(ID=seq(n_agents), 
                      strategy=sample(strategy_list, n_agents, prob = prop_strategy, replace=TRUE))

  output$strategy_function <- if_else(output$strategy
                                      %in% c("NCB", "CB", "RB", "WSLS", "SoftmaxTitTat"),
                                      output$strategy, "k_ToM") #All k-ToM's use the function k-ToM
  output$strategy_function <- if_else(output$strategy_function
                                      %in% c("NCB", "CB", "RB"),
                                      "simple", output$strategy_function) #NCB, CB and RB use the function simple
  return(output)
}

match_up <- function(agents_list, matchup_type){ 
  #Generate pairs based on matchup_type given as a string
  
  #INPUT
    #agents_list, a list of agents to be paired given as a character vector
    #matchup_type given as a character string
      #Possible choices
        #Random: An agents is randomly matched up with another agents
        #RR: Round robin matchup, each agent battles all other agents
  #OUTPUT
    #a matrix of the pairs
  
  if (matchup_type == "random"){ 
    matrix(sample(agents_list, length(agents_list), replace=F), ncol = 2) #pairs the participants randomly
  } else if (matchup_type == "RR"){
    t(combn(agents_list, 2)) #create all possible pairs
  } else {
    stop("Please chose a valid matchup_type")
  }
}
```

#Prepare agent function
```{r prepare}
prepare <- function(strategy_string){
  if (strategy_string == "NCB"){
    #very low prop. of choosing 1
    params <- list(list("prop" = inv.logit(rnorm(1, mean = logit(0.1), sd = 0.1))))
    
  } else if (strategy_string == "CB") {
    #very high prop. of choosing 1
    params <- list(list("prop" = inv.logit(rnorm(1, mean = logit(0.9), sd = 0.1))))
    
  } else if (strategy_string == "RB") {
    # prop. of approx. 50% of choosing 1
    params <- list(list("prop" = inv.logit(rnorm(1, mean = logit(0.5), sd = 0.1))))
    
  } else if (strategy_string == "WSLS") {
    params <- list(list("noise" = inv.logit(rnorm(1, mean = logit(0.1), sd = 0.1))))
    
  } else if (strategy_string == "SoftmaxTitTat") {
    params <- list(list("bavioral_temp" = inv.logit(rnorm(1, mean = logit(0.5), sd = 0.2))))
    
  } else if (grepl("-ToM", strategy_string)) { #if the strategy is a k-ToM
    k <- as.numeric(str_extract(strategy_string, "[0-9]"))
    params = prepare_kToM(k)
  } else {
    stop("Could not find strategy, try to check for spelling errors")
  }
  

  return(params)
}
```

#prepare k-ToM - placeholder
```{r}
## CURRENTLY AT THE BUTTON OF THE CODE 
 
```

#Functions
```{r functions}  
#contains a mixture of functions used for strategies
 
###_________________________ General _________________________###

#utility function
U <- function(a.self, a.op, player, p_matrix){ 
  # takes three arguments
  # a.self: choice of self
  # a.op: choice of opponnent
  # player: which side of the payof matrix is used. 0: first player, 1: second player
  # returns the point reward
  
  # get values from payoff matrix for player 1
  a_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # get values from payoff matrix for player 2
  a_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  #calculate reward
  (1-player) * #for player 1
    (  a.self * a.op * a_1 +           #if both choose 1 
       a.self * (1 - a.op) * b_1 +     #if you choose 1 and opponent chooses 0
       (1 - a.self) * a.op * c_1 +     #if you choose 0 and the opponent chooses 1
       (1 - a.self) * (1 - a.op) * d_1 #if both choose 0
       ) + 
  player * #for player 2
    (  a.self * a.op * a_2 +           #if both choose 1 
       a.self * (1 - a.op) * b_2 +     #if you choose 1 and opponent chooses 0
       (1 - a.self) * a.op * c_2 +     #if you choose 0 and the opponent chooses 1
       (1 - a.self) * (1 - a.op) * d_2 #if both choose 0
       )
}

#Expected payoff
payoff_difference <- function(p_op_1, player, p_matrix){
  p_op_1*(U(1, 1, player, p_matrix)-U(0, 1, player, p_matrix))+(1-p_op_1)*(U(1, 0, player, p_matrix)-U(0, 0, player, p_matrix))
}

#softmax functions
softmax <- function(e_payoff_diff, beta){ 
  # Takes two arguments
    # expected payoff difference
    # a beta also called bahaivoural temperature - higher make the agent choice more random
  # Returns probability self choosing 1
  1/(1 + exp(-(e_payoff_diff/beta)))
}



###______________________ ToM specific _______________________###

#Probability of opponnent a^op choosing 1
p_op_1_fun <- function(mu, variance, sigma){
  # takes in three arguments
    # mu is approximate mean of 0-ToM posterior distribution
    # variance is subjective uncertainty of the mean 
    # sigma is the volatility
  # returns probability of opponent choosing 1 
  inv.logit(mu/sqrt(1+(variance+sigma)*3/pi^2))
}

mu_update <- function(mu, variance, choice_op){
  # takes 3 arguments
    # mu from the round before
    # variance form the round before
    # choice_op, which is the opponents choice last round
  # returns an updated mu
  mu + variance * (choice_op - inv.logit(mu))
}

variance_update <- function(mu, variance, sigma) {
  # takes 3 arguments
    # mu from last round
    # variance from last round
    # sigma: (volatility) higher values mean lower updating
  #returns an updated variance
  1 / (1 / (variance + sigma) + inv.logit(mu) * (1 - inv.logit(mu)))
}

```

#Simple Agents functions
```{r simple agents}  
RB <- function(ID, player = NULL, p_matrix = NULL, last_round = NULL, params = agents_df$params[agents_df$ID == ID]){
  #Simple: Used for simple agents:
    #RB: Random choice with a bias 
    #CB: Cooperate with a bias
    #NCB: Never cooperate with a bias
  #INPUT
    #ID: Not used (dummy variable for consistency)
    #round_result: Not used (dummy variable for consistency)
    #params: a list of 1 element, where the element is the prop of choosing 1 
  #OUTPUT
    #Choice: 0 or 1
  
  #randomly select rabbit or stag (with a slight bias)
  rbinom(1, 1, prob = params[[1]]$prop)
}

WSLS <- function(ID, player = NULL, p_matrix = NULL, last_round, params = agents_df$params[agents_df$ID == ID]){   
  #WSLS: Win stay loose switch
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: noise
  #OUTPUT
    #Choice: 0 or 1
  
  noise <- params$WSLS$noise #get the noise parameter #MAKE IT SLOPE PARAMETER INSTEAD, ONE FOR LOOSING AND ONE FOR WINNING
  gain = last_round$points[last_round$ID == ID] #agent's gain last round
  mean_gain = (1 - player) * mean(p_matrix$a1_reward) + player * mean(p_matrix$a2_reward) # above mean gain: win, below mean gain: loss
  max_gain = (1 - player) * max(p_matrix$a1_reward) + player * max(p_matrix$a2_reward) # maximum possible gain
  min_gain = (1 - player) * min(p_matrix$a1_reward) + player * min(p_matrix$a2_reward) # minimum possible gain
  
  #Add the minimum value to all values to avoid negatives. Has no effect on the outcomes.
  gain = gain + min_gain
  mean_gain = mean_gain + min_gain
  max_gain = max_gain + min_gain
  min_gain = min_gain + min_gain
  
  if (class(last_round)[1] != "tbl_df") { #initial round or missed trial
    choice <- rbinom(1, 1, prob = 0.5) #make a random choice
  } else {
    if (gain - mean_gain > 0) { #if the player won last round
    
      # Input gain relative to maximum gain in a logistic function. Lower noise param randomizes. When noise->infinite the agent always stays if above mean score.
      probability_stay = 1 / (1 + exp(-noise * (gain/max_gain - mean_gain))) 
      stay = rbinom(1, 1, prob = probability_stay) #whether the agent stays
    
    } else { #if the player lost last round. if gain = mean_gain, probability will be 0.5
      
      #Input gain relative to 
      probability_switch = 1 / (1 + exp(-noise * (min_gain/gain  - mean_gain)))
      stay = rbinom(1, 1, prob = 1 - probability_switch) #whether the agent stays
    } 
    
  choice <-
    last_round$choice[last_round$ID == ID] * stay + #if staying equals last choice
    (1 - last_round$choice[last_round$ID == ID]) * (1 - stay) #if not staying equals opposite of last choice
  }
  
  return(choice)
}


SoftmaxTitTat <- function(ID, last_round, params = agents_df$params[agents_df$ID == ID]) {  
  #SoftmaxTitTat: A Tit for tat agent using a softmax function
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: the behaivioural temperature of the agent
  #OUTPUT
    #Choice: 0 or 1
  
  beta <- params$SoftmaxTitTat$bavioral_temp
  
  if (class(last_round)[1] != "tbl_df"){ #initial round or missed trial
    #Start by assuming you opponent will COOP - will make the agents every likely to COOP
    e_payoff_diff <- payoff_difference(1)
    choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  } else {
  e_payoff_diff <- payoff_difference(last_round$choice[last_round$ID != ID])
  choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  }
  
  return(choice)
}
```

#Reinforcement Learning #!# Not added
```{r reinforcement Learning}
#Not added
```

#k-ToM - placeholder
```{r k-ToM}  
##CURENTLY AT THE BUTTON OF THE CODE 
```

#Compete function
```{r compete}

compete <- function(pair, n_rounds = 10){   
  #INPUT
    #pair: a pair of participant to compete
    #n_rounds: number of rounds
  a1 <- pair[1]
  a2 <- pair[2]
  
  round_result <- NA
  
  for (round in 1:n_rounds){
      #calls the strategy function for each agent and gives it the ID (a1/a2) as a argument 
    a1_choice <- do.call(agents_df$strategy_function[agents_df$ID == a1], 
                         args = list(ID = a1, 
                                     last_round = round_result))
    a2_choice <- do.call(agents_df$strategy_function[agents_df$ID == a2], 
                           args = list(ID = a2, 
                                       last_round = round_result))
      
      #generate result
    result <- p_matrix[p_matrix$a1_choice == a1_choice & p_matrix$a2_choice == a2_choice,]
        
      #save results    
    round_result <- data_frame(ID = c(a1, a2),  
                               choice = c(a1_choice, a2_choice),
                               points = c(result$a1_reward, result$a2_reward), 
                               round_nr = round,
                               strategy = c(agents_df$strategy[agents_df$ID == a1],
                                            agents_df$strategy[agents_df$ID == a2]),
                               pair = paste(a1, a2, sep = "/"))
    if (round == 1){
      result_df <- round_result
    } else {
      result_df <- rbind(result_df, round_result)
    }
  }
  return(result_df)
}
```

#Running all the things
```{r simulation}  
  #Create agents
#The following list is meant as inputs for create_agents()
strategy_list <- c("NCB",  "CB",  "RB", "WSLS", "SoftmaxTitTat", "0-ToM", "1-ToM",  "2-ToM")
prop_strategy <- c(  0.0,   0.5,   0.0,    0.0,             0.0,     0.5,     .5,      0.0)

agents_df <- create_agents(n_agents = 10, 
              strategy = strategy_list, 
              prop = prop_strategy)

  #creating pairs
pairs <- match_up(agents_list = agents_df$ID, matchup_type = "random")

  #preparing agents
agents_df$params <- sapply(agents_df$strategy, prepare)

  #make the agents compete in the staghunt
result_list <- apply(pairs, 1, compete, n_rounds = 30) #using apply to apply compete() to pairs and adding n_rounds as an argument to compete()

  #bind the resulting dataframe by row
result_df <- result_list %>% bind_rows()

  #inspecting results
head(result_df, 20)

  #summing up the point for each partipant
sum_result <- result_df %>% 
  dplyr::group_by(ID, strategy) %>% 
  dplyr::summarise(total_points = sum(points)) 
sum_result
```

#Export data
```{r write csv}
write_csv(sum_result, path = "sum_result.csv")
write_csv(sum_result, path = "result_df.csv")
```


### ___ K-tom testcode ___ ###
```{r}
prepare_kToM <- function(k){
  behavioral_temp = inv.logit(rnorm(1, mean = logit(.5), sd = 0.2))#?# what is the values in the matlab code?
  sigma = inv.logit(rnorm(1, mean = logit(0.3), sd = 0.2)) #?# what is the values in the matlab code?
  
  if (k == 0){
    variance = rnorm(1, mean = 1.5, sd = 0.2) #?# what is the values in the matlab code?
    mu = inv.logit(rnorm(1, mean = logit(0.5), sd = 0.2))
    
    params <- list(behavioral_temp = behavioral_temp, 
                 variance = variance, 
                 mu = mu, 
                 sigma = sigma, 
                 k = k)
    
  } else if (k > 0){
    p_k <- rep(1/k, k) #probabilty for each possible sophistication level of the opponent 
                       #(note: 1-ToM's opponent == 0-ToM, e.g. p_k = 1) 
    mu <- rep(0.5, k) #equal chance of choosing both strategies (naive starting point)
    variance <- rnorm(k, mean = 1.5, sd = 0.2) #!# temporary - should be extracted from matlab 
    
      params <- list(behavioral_temp = behavioral_temp, 
                 variance = variance, 
                 mu = mu, 
                 sigma = sigma,
                 p_k = p_k, 
                 k = k)
  }
  params <- list(params)
  return(params)
}

update_pk <- function(p_k, choice_op, p_op_1){
  #eq. S4 p. ix (9) in Devaine, et al. (2017) appendix
  #calculates P(k)
  p_k <- choice_op* #if opponent choose 1
    ((p_k*p_op_1)/sum(p_k*p_op_1)) + 
    (1-choice_op)* #if opponent choose 0
    (p_k*(1-p_op_1)/sum(p_k*(1-p_op_1))) 
  
  return(p_k)
}

p_op_1_k_fun <- function(mu, variance){
  #INPUT
    #takes a mu and a variance (of opponent)
  #OUTPUT
    #p_op_1_k probability of opponent choosing 1 given k, e.g. P(c_op = 1|k') in loggodds
  
  #this calculation uses a semi-analytical approximation by Daunizeau, J. (2017)
  
  #constants
  a <- 0.205
  b <- -0.319
  c <- 0.781
  d <- 0.870
  
  #calculation
  tmp <- (mu + b * variance^c) / sqrt(1 + a * variance^d)
  p_op_1_k <- log(inv_logit(tmp))
  
  return(p_op_1_k)
}

k_ToM <- function(ID, last_round, params = agents_df$params[agents_df$ID == ID]){
    #k-ToM; A Theory of Mind agent in which the agent sophistication level is k
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of X element: ... #!# fill here
  #OUTPUT
    #Choice: 0 or 1
  print(ID)
  
  ###_____ FETCH VARIABLES _____###
  
  if (class(last_round)[1] == "tbl_df"){ #if not first round
    choice_self <- last_round$choice[last_round$ID == ID]
    choice_op <- last_round$choice[last_round$ID != ID]
  }
  
  k <- params[[1]]$k
  beta <- params[[1]]$behavioral_temp
  variance <- params[[1]]$variance
  mu <- params[[1]]$mu
  
  if (k == 0){ #0-ToM
    sigma <- params[[1]]$sigma #!# no sigma for k-ToM, k > 0?
  } else { #k > 0
    if (class(last_round)[1] != "tbl_df"){#if it is the first round...
      p_op_1_k_logodds <- p_op_1_k_fun(mu, variance)  #calculate P(c_op|k')
      p_op_1_k <- inv_logit(p_op_1_k_logodds)
    } else { #else fetch it
      p_op_1_k <- params[[1]]$p_op_1_k
    }
    p_k <- params[[1]]$p_k
  }
  
  ###_____ UPDATE BELIEFS _____###
  if (class(last_round)[1] == "tbl_df"){ #not first roundfirst round
    if (k == 0){ #0-ToM
      variance <- variance_update(mu, variance, sigma) 
      mu <- mu_update(mu, variance, choice_op)
    } else { #k > 0
      #!# vi skal lige være helt sikre på rækkefølgen af udregningerne
      #!# vi skal være helt sikre på hvad W/df gør (pt. er det antaget at være et neutral element)
      mu <- mu + p_k * variance * (choice_op - p_op_1_k) #posterior mean (mu)
      variance <- 1 / (1 / variance) + p_k * p_op_1_k * (1 - p_op_1_k) #posterior variance 
      p_k <- update_pk(p_k, choice_op, p_op_1_k) #P(k'), prob. of o. having sophistication level k'
      p_op_1_k_logodds <- p_op_1_k_fun(mu, variance) #P(c_op = 1 | k'), prob. of op. choosing 1 given sophistication level, k' (in logodds)
      p_op_1_k <- inv_logit(p_op_1_k_logodds) #!# should this always be in % 
    }
  }
  
  ###_____ SAVE PARAMS _____###
    #<<- means assign to the global variabel (similar to af assign())
  agents_df$params[agents_df$ID == ID][[1]]$variance <<- variance
  agents_df$params[agents_df$ID == ID][[1]]$mu <<- mu
  
  if (k > 0){
    agents_df$params[agents_df$ID == ID][[1]]$p_k <<- p_k
    agents_df$params[agents_df$ID == ID][[1]]$p_op_1_k <<- p_op_1_k
    print("it did it")#for testing
  }
  
  ###_____ CALCULATE RESPONSE _____###
  if (k == 0){ #0-ToM
    p_op_1 <- p_op_1_fun(mu, variance, sigma) #estimated probability of opponent chosing 1 in %
  } else { #k > 0
    p_op_1 <- p_k*p_op_1_k #the probality of k multiplied by the probality of chosing one given k  
  }
  
  e_payoff_diff <- payoff_difference(p_op_1) #estimated payoff difference
  p_self_1 <- softmax(e_payoff_diff, beta) #probability of self chosing 1
  choice <- rbinom(1, 1, p_self_1)

  return(choice)
}

```

#testing code
```{r} 
agents_df <- data_frame(ID = seq(4), 
                        strategy = c("1-ToM", "3-ToM", "2-ToM", "3-ToM"),
                        strategy_function = rep("k_ToM", 4),
                        params = NA
                        )
agents_df

agents_df$params[1] <- prepare_kToM(1)
agents_df$params[2] <- prepare_kToM(3)
agents_df$params[3] <- prepare_kToM(2)
agents_df$params[4] <- prepare_kToM(3)
agents_df$params
round(agents_df$params[[2]]$p_k , 3)
round(agents_df$params[[4]]$p_k, 3)

temp  <- compete(c(1,2), n_rounds = 1000)
temp1 <- compete(c(3,4), n_rounds = 1000)


agents_df$params[[1]]

```

### ___ Random Code ___ ###

#Riccardo's code
```{r RF code} 
d<-d[complete.cases(d),]
d$Left=as.numeric(d$Left)
d$Handedness=1
d$Handedness[d$Left==0]=-1
d$Success=1
d$Success[d$IndividualPayoff==0]=0
d$Failure=0
d$Failure[d$IndividualPayoff==0]=1
d$X1=d$Handedness*d$Success
d$X2=d$Handedness*d$Failure
d$StayBias=c(NA,d$X1[1:(nrow(d)-1)])
d$LeaveBias=-c(NA,d$X2[1:(nrow(d)-1)])

WinStay <-
  brm(
    Left ~ 0 + StayBias + LeaveBias + (0 + StayBias + LeaveBias | Subject),
    d,
    family = bernoulli,
    cores = 2,
    chains = 2,
    control = list(adapt_delta = 0.99, max_treedepth = 20),
    iter = 3e3
  )



#Simulating values for beta (behavioral temperature)
x=data.frame(V=rep(NA,100),B=rep(NA,100),p=rep(NA,100))
n=1
for (V in seq(from=0, to=1, by=0.1)){
  for (B in seq(from=0, to=3, by=0.5)){
    print(n)
    x$B[n]=B
    x$V[n]=V
    x$p[n]=1 / (1+exp(-V/B))
    n=n+1
  }
}
ggplot(x, aes(V,p,group=B,color=B))+geom_smooth()


```

### ___ Old Code ___ ###

  #I don't dare to delete it yet

#0-ToM
```{r 0-ToM}

###________________________ ToM agents _______________________###
 
  #see Devaine, et al. (2017) https://doi.org/10.1371/journal.pcbi.1005833

#k=0
ToM0 <- function(ID){ 
  if (is.na(part_df$param[part_df$ID == ID])){ 
    #generates individual differences / starting point
    part_df$param[part_df$ID == ID] <- list(c(bavioral_temp = rnorm(1, mean = .5, sd = 0.2), 
                                              variance = rnorm(1, mean = 1.5, sd = 0.3),
                                              mu = rnorm(1, mean = 0.5, sd = 0.2), #this is in logodds #!# set it up so people are slightly more likely to COOP
                                              sigma = inv.logit(rnorm(1, mean = logit(0.3), sd = 0.2)) 
                                              ))

  }
  #fetching info from the participant with the corresponding ID
  beta <- part_df$param[part_df$ID == ID][[1]]['bavioral_temp']
  variance <- part_df$param[part_df$ID == ID][[1]]['variance']
  mu <- part_df$param[part_df$ID == ID][[1]]['mu']
  sigma <- part_df$param[part_df$ID == ID][[1]]['sigma'] 
  
  #getting info from the last round
  current_pair <- paste(pairs[pair,], collapse = "/")
  last_round <- try(result_df[result_df$pair == current_pair & result_df$round_nr == n_round-1,], 
                    silent = T) 
  
  if ((class(last_round)[1] == "try-error" | empty(last_round)) == FALSE){ #if the data isn't empty and there was no error reading it (e.g. if it was the very first round) - update the variable (this will almost always happen with the exception being the start of a round)
    
    a.op <- last_round$choice[last_round$ID != ID] #opponent last choice
  
    #update parameters based on opponnents previous choices 
    variance <- variance_update(mu, variance, sigma) #?# peter: var det variance der skulle være først?
    mu <- mu_update(mu, variance, a.op)
    
    #save new values of variance and mu
    part_df$param[part_df$ID == ID][[1]]['variance'] <- variance
    part_df$param[part_df$ID == ID][[1]]['mu'] <- mu
  }
  
  
  #calculate response
  e_prop_a.op1 <- prop_a.op1(mu, variance, sigma) #estimated probability of opponent chosing 1 in prop
  e_payoff_diff <- payoff_difference(e_prop_a.op1) #estimated payoff difference
  prop_a.self1 <- softmax(e_payoff_diff, beta) #probability of self chosing 1
  prop_a.self1
  
  choice <- rbinom(1, 1, prop_a.self1)
  
  
  
  return(choice)
}
```


#prepare k-ToM
```{r}
 
###_______ Help functions  _______###

sizeXrec <- function(i, P){
  #Returns the number of k-ToM hidden states
    #Note from source: Marie Devaine wrote this in November 2015
  #INPUT
    #i: k-ToM's sophistication level
    #n_tot_par: total nb of evol/obs params
  #OUTPUT
    #n_x: number of k-ToM's hidden sates
  ##YOU ARE HERE##
}

###_______ The actual function _______###

prepare_kToM <- function(k){
  #INPUT
    #Sofistication level
  #OUTPUT
    #Params: a list of all starting parameters of the agents
  #NOTE:
    #This code is a translation of the matlab code: prepare_kToM.m and prepare_agents.m
    #see Devaine, et al. (2017) https://doi.org/10.1371/journal.pcbi.1005833
    # #%# signifies not implented og currently implementing
  
  
  
  theta = -2 #-log(2); % (log-) prior volatility 
  #%#       [options,dim] = prepare_kToM(K,game,role,0);
  #%#       f = @f_kToM;
  #%#       g = @g_kToM;
  phi = c(-1, 0) # (log-) behavioral temperature and bias
  #%#       inF = options.inF;
  #%#       inG = options.inG;
  #%#       x0 = options.priors.muX0;
  #%#       inF.dummyPar = [1;0;0]
  n_obs_par = 2 #Number of observation parameters
  n_evo_par = 1 #Number of evolution parameters
  n_tot_par = n_obs_par + n_evo_par #Total number og parameters #KCE: can just be 3
  
  #%#         inG = inF; #?# RF, what is this exactly?
  #%#         inG.indlev = defIndlev(K,NtotPar); % states' indexing
  #defIndlev.m:
  ind_level <- list() #individual levels
    #Note: if k == 0 this list is empty as 0-ToM oppononent has no individual levels
  if (k > 0){
    for (i in 1:k){
      ind_level$op_k[i]=i-1
      
      #sizeXi = sizeXrec(i-1,NtotPar)
      #sizeXrec.m
        #Number of hidden states (x)
      if (i-1 == 0){ #e.g. if the opponent is a 1-ToM
        n_x = 2 #number of hidden states (x)
      } else {
        
      }
      
      
      
      
      
    }
    
  }
  
  
  #%#         inG.npara = NtotPar;
  #%#         options.inF = inF;
  #%#         options.inG = inG;
  
  prior_mu <- list()
  
  if (k>0){
    for (i in 1:k){
      # % prior (log-) volatility = -1 (opponent = efficient learner)
      prior_mu$ind_level[i] = 0
      #   % "forgetting" of opponent's sophistication
      #   if inF.diluteP==1
      #       indDil = inG.indlev(j).Par(3); % index of hidden-state encoding E[invsigm dilut. coef.]
      #       options.priors.muX0(indDil) = -2;
      #   end
      #   % behavioural temperature = -1 (opponent = exploitative decider)
      #   indTemp = inG.indlev(j).Par(2*inF.indParev+1); % index of hidden-state encoding E[log temp]
      #   options.priors.muX0(indTemp) = 0;%-1;
    }
    
  }
  
  
  
  
  params <- list(list())
  
  return(params)
}
```


#k-ToM #!# WIP
```{r k-ToM}
  #see Devaine, et al. (2017) https://doi.org/10.1371/journal.pcbi.1005833



k-ToM <- function(ID, last_round, params = agents_df$params[agents_df$ID == ID]){
  strategy_string <- agents_df$strategy[agents_df$ID == ID]
  k <- str_extract(strategy_string, "[0-9]") #agents sofistication level

  
  
}

  #NOTE: The following is a translation of recToMfunction.m
  
#k-ToM
  #input original: x (hidden states, see inF.indlev), P (evolution params), u (inputs e.g. previous choice), inF (input structure - see prepape_kToM.m)
  #output original: fx (updated hidden variables), indlevel (recursive states indexing see defIndlev.m)

#input this script, ID

level <- 1 # depth of recursive beliefs (k=level) #!# currently dummycoded k=1

#metaweight (w=1) is currently assumed to be 1
#diluteP is currently assumed to be 1 or 0 (whichever is the neutral element) - as we assume opponent params don't drift over time (correctly so in this case)

if (level == 0){
  print("not made yet") break
} else {
  #run k-ToM
  #fetch stuff #needs stuff #!# add this
  
  ### 1) Update P(k'): agent's belief regarding opponent's level. k-ToM maintains parallel prediction P(c.op = 1|k') about opponents next more (c.op) depending on opponent's level (k')
  
  if (first_trial==T){ #if it is the first trial #!# dummycoded
    Pk <- 1/level*rep(1, level)#max entropic belief about opponent's level e.g. probability of opponent having k-level is equal when no information is known (starts at k=0). Note: 1-ToM's opponent = 0-ToM 
    if (level > 1){
      print("not made yet") break
    }
    
  } else { 
    if (level > 1) { # k >= 2
      print("not made yet") break
    }
  }  
  ### 2) Update P(theta|k'), the agent's belief about her opponnent's params. (Note: that we do not assume the opponents beliefs to drift)
    
  ## 2.1) update E[theta|k'] & V[theta|k'] (opponents mu and variance)
  for (j in 1:level){ # loop through the admissable levels of the opponent (k' = j-1) #?# maybe a more meaningful name than j?
    j.Pk <- Pk[j]
    #in.f   <-- not really sure here (get back to it - marked with ** on sheet)
    if (first_trial==T){ #if it is the first trial #!# dummycoded
      #don't update parameters
      #!# add stuff here
    } else { #VB-Laplace (variational bayes) update rule for k-ToM's belief 
      #needed here: 
        #Par_k: K-ToM's prior mean and variance on his opponent's params
        #theta: k-ToM's assumed log-volatility of his opponent's params
        #ot: k-ToM's opponents
        #in: precalculated intermediary variable
      #NB: VB update of opponent's params does not account for potential indentifiability issues between params (cf. mean-field assumption)
      Pk = j.prop_k #synes det her er double trouble men der er sikkert en grund
      prop_1.op <- inv.logit()#P(c.op = 1|k) 
      V0 <- exp(Par_k(indV)) #prior variance
      Vu <- 1/((1/V0)+Pk*prop_1.op*(1-prop_1.op))  #Posterior variance #eq. s4
      E0 <- indMu
      Eu <- E0 + Pk * Vu * (c.op-prop_1.op)    #posterior mean
      #output results
      indV <- log(Vu)
      indMu <-  Eu
    }
  }
      
    
}
```





