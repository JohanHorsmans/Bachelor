---
title: "ToM_Recursion"
author: "Kenneth C. Enevoldsen"
date: "11/15/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Packages and WD
```{r packages and wd} 
#devtools::install_github("thomasp85/patchwork")
pacman::p_load(plyr, tidyverse, raster, reshape2, knitr, brms, boot, rethinking, groupdata2, patchwork)
```

#Payoff matrices
```{r Payoff matrix} 
### Payoff matrices for different games ###

custom_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 0, 0, 0), 
             a2_reward = c(0, 0, 0, 0))

staghunt_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(5, 3, 0, 3), 
             a2_reward = c(5, 0, 3, 3))

penny_cooperative_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(1, 0, 0, 1), 
             a2_reward = c(1, 0, 0, 1))

penny_competitive_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 1, 1, 0), 
             a2_reward = c(1, 0, 0, 1))

prisoner_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(2, 3, -1, 0), 
             a2_reward = c(2, -1, 3, 0))
```


#Prepare agents functions
```{r prepare}
prepare <- function(strategy_string, RB_prop = 0.5){
  #INPUT
    #strategy_string: A string of the strategy the agents should apply
    #rb_prop: - 
    #
  #OUTPUT
    #The prepared parameters of the agent.
  
  if (strategy_string == "RB") {
    # prop. of approx. 50% of choosing 1 assuming no other values is given
    params <- list(list("prop" = inv.logit(rnorm(1, mean = logit(RB_prop), sd = 0.1))))
    
  } else if (strategy_string == "WSLS") {
    params <- list(list("noise" = inv.logit(rnorm(1, mean = logit(0.9), sd = 0.1))))
    
  } else if (strategy_string == "SoftmaxTitTat") {
    params <- list(list("bavioral_temp" = inv.logit(rnorm(1, mean = logit(0.99), sd = 0.2))))
    
  } else if (grepl("-ToM", strategy_string)) { #if the strategy is a k-ToM
    k <- as.numeric(str_extract(strategy_string, "[0-9]"))
    params = prepare_kToM(k)
  } else {
    stop("Could not find strategy, try to check for spelling errors")
  }

  return(params)
}

prepare_df <- function(agent_df_unprep, args = NULL){
  #INPUT
    #agent_df a tibble prepared with prepare_agents
    #args, a character vector of arguments to pass two the prepare function
      #One string should be given pr. agents, an empty string simply used deffaults
      #Note it prepared player1 then Player2 so all player1's should be first in the list
  #OUTPUT
    #The agent_df with prepared features
  
  p1_args <- args[1:(nrow(agent_df_unprep))]
  p2_args <- args[-(1:(nrow(agent_df_unprep)))]

    #adding commas for the eval expression, should only be there is there is aditional arguments
  if_else(nchar(p1_args) != 0 ,paste(",", p1_args), p1_args) 
  ifelse(nchar(p2_args) != 0 ,paste(",", p2_args), p2_args)
  
    #calling the prepare function
  for (i in 1: length(p1_args)){
    agent_df_unprep$params_p1[i] <- eval(parse(text = 
                                  paste("prepare(strategy_string = '", agent_df_unprep$player1[i], "' ,", p1_args[i], ")", sep = "")))
  }
  for (i in 1: length(p2_args)){
    agent_df_unprep$params_p2[i] <- eval(parse(text = 
                                  paste("prepare(strategy_string = '", agent_df_unprep$player2[i], "' ,", p2_args[i], ")", sep = "")))
  }
  
  
  return(agent_df_unprep)
}
```

#create_agents functions
```{r create_agents}
create_agents <- function(strategies, match_up = "RR", prepare_with_defaults = T){
  #strategies: a list of strategies given as a vector
    #implemented agent strategies
        #RB: Random Bias
        #WSLS: Win Stay Loose Switch
        #SoftmaxTitTat: A tit for tat using a softmax function as well as an utility function
        #k-ToM: a k-level theory of mind agent (e.g. k=0 => 0-ToM) 
          #Note that you input the desired k-level in place of k, e.g. 1-ToM
  #match_up:
    #implemented match up types include
      #RR: Round robin, every strategy in the strategies input matched up against every other strategy
      #random: The strategies in the strategies input is randomly matched up
      #half_half: the first half is matched up with the second half
  #prepare_with_defaults:
    #if TRUE prepares the agents using their deffault setup
    #if FALSE does not prepare the agents, in this case we refer to the prepare() function
  #OUTPUT
    #a dataframe with the created agent matched up and prepared if prepare_with_defaults is set to TRUE
  
  if (match_up == "RR"){
    output <- as_data_frame(t(combn(strategies, 2))) %>% dplyr::select(player1 = V1, player2 = V2)
  } else if (match_up == "half_half" & length(strategies) %% 2 == 0){
    output <- as_data_frame(matrix(strategies, ncol = 2)) %>% dplyr::select(player1 = V1, player2 = V2)
  } else if (match_up == "random" & length(strategies) %% 2 == 0){
    output <- as_data_frame(matrix(sample(strategies, replace=F), ncol = 2)) %>% dplyr::select(player1 = V1, player2 = V2)
  } else {
    stop("Please input valid match_up type 
         or if using 'half_half' or 'random' make sure that your strategies input has an even length")
  }
  
  if (prepare_with_defaults){
    output$params_p1 <- sapply(output$player1, prepare)
    output$params_p2 <- sapply(output$player2, prepare)
  }

  return(output)
}
```

#Functions
```{r functions}
#contains a mixture of functions used for strategies
 
###_________________________ General _________________________###

#utility function
U <- function(a_self, a_op, player, p_matrix){ 
  #INPUT
    #a_self: choice of self
    #a_op: choice of opponnent
    #player: which side of the payof matrix is used. 0: first player, 1: second player
    #returns the point reward
  #OUTPUT
    #The utility of of the given choices for the player.
  
  # get values from payoff matrix for player 1
  a_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # get values from payoff matrix for player 2
  a_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # calculate reward
  (1-player) * #for player 1
    (  a_self * a_op * a_1 +           #if both choose 1 
       a_self * (1 - a_op) * b_1 +     #if you choose 1 and opponent chooses 0
       (1 - a_self) * a_op * c_1 +     #if you choose 0 and the opponent chooses 1
       (1 - a_self) * (1 - a_op) * d_1 #if both choose 0
       ) + 
  player * #for player 2
    (  a_self * a_op * a_2 +           #if both choose 1 
       a_self * (1 - a_op) * b_2 +     #if you choose 1 and opponent chooses 0
       (1 - a_self) * a_op * c_2 +     #if you choose 0 and the opponent chooses 1
       (1 - a_self) * (1 - a_op) * d_2 #if both choose 0
       )
}

#Expected payoff
payoff_difference <- function(p_op_1, player, p_matrix){
  p_op_1*(U(1, 1, player, p_matrix)-U(0, 1, player, p_matrix))+(1-p_op_1)*(U(1, 0, player, p_matrix)-U(0, 0, player, p_matrix))
}

#softmax functions
softmax <- function(e_payoff_diff, beta){ 
  #INPUT
    # expected payoff difference
    # a beta also called bahaivoural temperature - higher make the agent choice more random
  #OUTPUT
    #Returns the probability self choosing 1
  1/(1 + exp(-(e_payoff_diff/beta)))
}



###______________________ ToM specific _______________________###

#Probability of opponnent a_op choosing 1
p_op_1_fun <- function(mu, variance, sigma){
  #INPUT
    # mu is approximate mean of 0-ToM posterior distribution
    # variance is subjective uncertainty of the mean 
    # sigma is the volatility
  #OUTPUT
    #returns probability of opponent choosing 1 
  inv.logit(mu/sqrt(1+(variance+sigma)*3/pi^2)) #example inv.logit(0/sqrt(exp(0)+(1+(-1))*3/pi^2))
}

mu_update <- function(mu, variance, choice_op){
  #INPUT
    # mu from the round before
    # variance form the round before
    # choice_op, which is the opponents choice last round
  #OUTPUT
    # returns an updated mu
  mu + variance * (choice_op - inv.logit(mu))
}

variance_update <- function(mu, variance, sigma) {
  #INPUT
    # mu from last round
    # variance from last round
    # sigma: (volatility) higher values mean lower updating
  #OUTPUT
    # returns an updated variance
  1 / (1 / (variance + sigma) + inv.logit(mu) * (1 - inv.logit(mu)))
}

update_pk <- function(p_k, choice_op, p_op_1){
  #eq. S4 p. ix (9) in Devaine, et al. (2017) appendix
  #calculates P(k)
  p_k <- choice_op* #if opponent choose 1
    ((p_k*p_op_1)/sum(p_k*p_op_1)) + 
    (1-choice_op)* #if opponent choose 0
    (p_k*(1-p_op_1)/sum(p_k*(1-p_op_1))) 
  
  return(p_k)
}

p_op_1_k_fun <- function(mu, variance){
  #INPUT
    #takes a mu and a variance (of opponent)
  #OUTPUT
    #p_op_1_k probability of opponent choosing 1 given k, e.g. P(c_op = 1|k') in loggodds
  
  #this calculation uses a semi-analytical approximation by Daunizeau, J. (2017)
  
  #constants
  a <- 0.205
  b <- -0.319
  c <- 0.781
  d <- 0.870
  
  #calculation
  tmp <- (mu + b * variance^c) / sqrt(1 + a * variance^d)
  p_op_1_k <- log(inv_logit(tmp))
  
  return(p_op_1_k)
}

```

#Simple Agents functions
```{r simple agents}  
###Random Choice with a Bias 
RB <- function(params, hidden_states = NULL, player = NULL, p_matrix = NULL, choice_self = NULL, choice_op = NULL, return_hidden_states = F){ 
  #RB: Random choice with a bias 
  #INPUT
    #params: a list of 1 element, where the element is the prop of choosing 1 
  #OUTPUT
    #Choice: 0 or 1
    #Updated parameter: This is for consistency, but is in practice not used
  
  #randomly select rabbit or stag (with a slight bias)
  choice <- rbinom(1, 1, prob = params[[1]]$prop)
  
  if (return_hidden_states == T){
    return(list(choice = choice, hidden_states = hidden_states))
  } else {
    return(choice)
  }
}



###Win-Stay-Loose-Switch
WSLS <- function(params, hidden_states = NULL,  player, p_matrix, choice_self, choice_op){   
  #WSLS: Win stay loose switch
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: noise
  #OUTPUT
    #Choice: 0 or 1
  
  if (is.null(choice_op)) { #initial round or missed trial
    choice <- rbinom(1, 1, prob = 0.5) #make a random choice
  }
  
  noise <- params[[1]]$noise #get the noise parameter #!# MAKE IT SLOPE PARAMETER INSTEAD, ONE FOR LOOSING AND ONE FOR WINNING
  gain = p_matrix[p_matrix$a1_choice == choice_self ]
    last_round$points[last_round$ID == ID] #agent's gain last round
  mean_gain = (1 - player) * mean(p_matrix$a1_reward) + player * mean(p_matrix$a2_reward) # above mean gain: win, below mean gain: loss
  max_gain = (1 - player) * max(p_matrix$a1_reward) + player * max(p_matrix$a2_reward) # maximum possible gain
  min_gain = (1 - player) * min(p_matrix$a1_reward) + player * min(p_matrix$a2_reward) # minimum possible gain
  
  #Add the minimum value to all values to avoid negatives. Add 1 to all values to avoid 0's. Has no effect on the outcomes.
  gain = 1 + gain + min_gain
  mean_gain = 1 + mean_gain + min_gain
  max_gain = 1 + max_gain + min_gain
  min_gain = 1 + min_gain + min_gain
  
  else {
    if (gain - mean_gain > 0) { #if the player won last round
    
      # Input gain relative to maximum gain in a logistic function. Lower noise param randomizes. When noise->infinite the agent always stays if above mean score.
      probability_stay = 1 / (1 + exp(-noise * (gain/max_gain - mean_gain))) 
      stay = rbinom(1, 1, prob = probability_stay) #whether the agent stays
    
    } else { #if the player lost last round. if gain = mean_gain, probability will be 0.5
      
      #Input gain relative to 
      probability_switch = 1 / (1 + exp(-noise * (min_gain/gain  - mean_gain)))
      stay = rbinom(1, 1, prob = 1 - probability_switch) #whether the agent stays
    } 
    
  choice <-
    last_round$choice[last_round$ID == ID] * stay + #if staying equals last choice
    (1 - last_round$choice[last_round$ID == ID]) * (1 - stay) #if not staying equals opposite of last choice
  }
  
  return(choice)
}

### Softmax Tit for Tat
SoftmaxTitTat <- function(params, hidden_states = NULL, player, p_matrix, choice_self = NULL, choice_op, return_hidden_states = F){  
  #SoftmaxTitTat: A Tit for tat agent using a softmax function
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: the behaivioural temperature of the agent
  #OUTPUT
    #Choice: 0 or 1
  
  beta <- params[[1]]$bavioral_temp
  c_op <- choice_op
  
  if (is.null(c_op)){ #initial round or missed trial
    #Start by assuming you opponent will COOP - will make the agents every likely to COOP
    e_payoff_diff <- payoff_difference(1, player, p_matrix)
    choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  } else {
  e_payoff_diff <- payoff_difference(c_op,  player, p_matrix)
  choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  }
  
  if (return_hidden_states == T){
    return_hidden_states <- return_hidden_states #does not update params
    return(list(choice = choice, hidden_states = hidden_states))
  } else {
    return(choice)
  }
}
```

#Compete function
```{r compete}
compete_all <- function(agent_df, p_matrix, n_rounds = 10){ 
  #call the compete function for each pair in the agent_df
  
  #INPUT
    #agent_df: A tibble (dataframe) of prepared agents resulting from create_agents()
    #n_rounds: number of rounds
    #p_matrix: The payoff matrix of the game you want them to be playing
  #OUTPUT
    #The results of the competitions as a tibble
  
  result_list <- apply(agent_df, 1, compete, n_rounds = n_rounds, p_matrix = p_matrix)
  result_df <- result_list %>% bind_rows()
  
  return(result_df)
}


compete <- function(player1, player2, p1_params, p2_params, p_matrix, n_rounds = 10){ 
  
    #this is for when the compete_all function is used - it simply unpacks the apply function
  if (class(player1) == "list"){
    p2_hidden_states <- list(player1[[6]])
    p1_hidden_states <- list(player1[[5]])
    p2_params <- list(player1[[4]])
    p1_params <- list(player1[[3]])
    player2 <- player1[[2]]
    player1 <- player1[[1]]
    
    print(p1_params)
    
  }
  
    #setting to NULL to signify it is the first round
  p1_choice = NULL
  p2_choice = NULL
  
    #fetch strategy function - just the player name unless it is a k-ToM
  strat_p1 <- if_else(grepl("-ToM", player1), "k_ToM", player1)
  strat_p2 <- if_else(grepl("-ToM", player2), "k_ToM", player2)
  
  for (round in 1:n_rounds){
    
      #calls the strategy function for each agent and gives it the player as a argument 
    p1 <- do.call(strat_p1, 
                         args = list(params = p1_params,
                                     player = 0,
                                     hidden_states = p1_hidden_states,
                                     p_matrix = p_matrix,
                                     choice_self = p1_choice,
                                     choice_op = p2_choice,
                                     return_hidden_states = T
                                     ))
  
    p2 <- do.call(strat_p2, 
                           args = list(params = p2_params,
                                       hidden_states = p2_hidden_states, 
                                       player = 1,
                                       p_matrix = p_matrix,
                                       choice_self = p2_choice,
                                       choice_op = p1_choice,
                                       return_hidden_states = T
                                       ))
        
    
    
      #generate result
    result <- p_matrix[p_matrix$a1_choice == p1$choice & p_matrix$a2_choice == p1$choice,]
        
    
    #results for next round 
    p1_hidden_states <- p1$hidden_states
    p2_hidden_states <- p2$hidden_states
    p1_choice <- p1$choice
    p2_choice <- p2$choice
    
    
    #save results
    round_result <- data_frame(player = c(player1, player2),  
                               choice = c(p1$choice, p2$choice),
                               points = c(result$a1_reward, result$a2_reward), 
                               round_nr = round, 
                               pair = paste(player1, player2, sep = " / "),
                               hidden_states = c(p1$hidden_states, p2$hidden_states)
                               )
  
    if (round == 1){
      result_df <- round_result
    } else {
      result_df <- rbind(result_df, round_result)
    }
  }
  return(result_df)
}

```


#Simulation
```{r simulation}  
#Preperation with deffaults
strategies = c("RB", "WSLS", "SoftmaxTitTat", "0-ToM", "1-ToM", "2-ToM", "3-ToM")
 
agent_df <- create_agents(strategies, match_up = "RR", prepare_with_defaults = T)
agent_df

#Preperation without specified features
strategies_unprep = c("RB", "RB", "SoftmaxTitTat", "SoftmaxTitTat")

agent_df_unprep <- create_agents(strategies_unprep, match_up = "half_half", prepare_with_defaults = F)
agent_df_unprep

args_list <- c("RB_prop = 0.1", "RB_prop = 0.9", "", "")#a char vector to pass as arguments for the prepare_df function
agent_df <- prepare_df(agent_df_unprep = agent_df_unprep, args = args_list)

```

#Testcode
```{r testcode}
strategies_unprep = c("RB", "SoftmaxTitTat")

agent_df_unprep <- create_agents(strategies_unprep, match_up = "half_half", prepare_with_defaults = F)

args_list <- c("RB_prop = 0.5", "")#a char vector to pass as arguments for the prepare_df function
agent_df <- prepare_df(agent_df_unprep = agent_df_unprep, args = args_list)
agent_df

compete(agent_df$player1, 
        agent_df$player2, 
        agent_df$params_p1, 
        agent_df$params_p2, 
        p_matrix = penny_competitive_matrix,
        n_rounds = 10
        )

compete_all(agent_df, n_rounds = 30, 
        p_matrix = penny_competitive_matrix)



player = agent_df$player2
params = agent_df$params_p2
```















