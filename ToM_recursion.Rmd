---
title: "ToM_Recursion"
author: "Kenneth C. Enevoldsen"
date: "11/15/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Packages and WD
```{r packages and wd} 
#devtools::install_github("thomasp85/patchwork")
pacman::p_load(plyr, tidyverse, raster, reshape2, knitr, brms, boot, rethinking, groupdata2, patchwork)
```

#Payoff matrices
```{r Payoff matrix} 
### Payoff matrices for different games ###

custom_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 0, 0, 0), 
             a2_reward = c(0, 0, 0, 0))

staghunt_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(5, 3, 0, 3), 
             a2_reward = c(5, 0, 3, 3))

penny_cooperative_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(1, 0, 0, 1), 
             a2_reward = c(1, 0, 0, 1))

penny_competitive_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 1, 1, 0), 
             a2_reward = c(1, 0, 0, 1))

prisoner_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(2, 3, -1, 0), 
             a2_reward = c(2, -1, 3, 0))
```


#Prepare agents functions
```{r prepare}
prepare_kToM <- function(k){ 
  #!# note in the following the sd=0, but the intention is to add it using an argument
  behavioral_temp = rnorm(1, mean = 0.37, sd = 0.0) #note: exp(-1) = 0.37
  sigma = rnorm(1, mean = 0.14, sd = 0.0) #, note exp(-2) = 0.14 #!# assumed from matlab code where theta = -2
  
  if (k == 0){
    variance = rnorm(1, mean = 1, sd = 0.0) #note exp(0) = 1
    mu = rnorm(1, mean = 0.5, sd = 0.0) #note inv.logit(0.5) = 0
    
    params <- list(behavioral_temp = behavioral_temp, 
                 variance = variance, 
                 mu = mu, 
                 sigma = sigma, 
                 k = k)
    
  } else if (k > 0){
    p_k <- rep(1/k, k) #probabilty for each possible sophistication level of the opponent 
                       #(note: 1-ToM's opponent == 0-ToM, e.g. p_k = 1) 
    mu <- rep(0.5, k) #equal chance of choosing both strategies (naive starting point)
    variance <- rnorm(k, mean = 1, sd = 0.0)
    
      params <- list(behavioral_temp = behavioral_temp, 
                 variance = variance, 
                 mu = mu, 
                 sigma = sigma,
                 p_k = p_k, 
                 k = k)
  }
  params <- list(params)
  return(params)
}

prepare <- function(strategy_string, RB_prop = 0.5){
  #INPUT
    #strategy_string: A string of the strategy the agents should apply
    #rb_prop: Probability of opponenent choosing 1
  #OUTPUT
    #The prepared parameters of the agent.
  
  if (strategy_string == "RB") {
    # prop. of approx. 50% of choosing 1 assuming no other values is given
    params <- list(list("prop" = inv.logit(rnorm(1, mean = logit(RB_prop), sd = 0.1))))
    
  } else if (strategy_string == "WSLS") {
    params <- list(list("noise" = inv.logit(rnorm(1, mean = logit(0.9), sd = 0.1))))
    
  } else if (strategy_string == "SoftmaxTitTat") {
    params <- list(list("bavioral_temp" = inv.logit(rnorm(1, mean = logit(0.99), sd = 0.2))))
    
  } else if (grepl("-ToM", strategy_string)) { #if the strategy is a k-ToM
    k <- as.numeric(str_extract(strategy_string, "[0-9]"))
    params = prepare_kToM(k)
  } else {
    stop("Could not find strategy, try to check for spelling errors")
  }
  return(params)
}
```

#create_agents functions
```{r create_agents}
create_agents <- function(strategies, match_up = "RR", prepare_with_defaults = T){
  #strategies: a list of strategies given as a vector
    #implemented agent strategies
        #RB: Random Bias
        #WSLS: Win Stay Loose Switch
        #SoftmaxTitTat: A tit for tat using a softmax function as well as an utility function
        #k-ToM: a k-level theory of mind agent (e.g. k=0 => 0-ToM) 
          #Note that you input the desired k-level in place of k, e.g. 1-ToM
  #match_up:
    #implemented match up types include
      #RR: Round robin, every strategy in the strategies input matched up against every other strategy
      #random: The strategies in the strategies input is randomly matched up
      #half_half: the first half is matched up with the second half
  #prepare_with_defaults:
    #if TRUE prepares the agents using their deffault setup
    #if FALSE does not prepare the agents, in this case we refer to the prepare() function
  #OUTPUT
    #a dataframe with the created agent matched up and prepared if prepare_with_defaults is set to TRUE
  
  if (match_up == "RR"){
    output <- as_data_frame(t(combn(strategies, 2))) %>% dplyr::select(player1 = V1, player2 = V2)
  } else if (match_up == "half_half" & length(strategies) %% 2 == 0){
    output <- as_data_frame(matrix(strategies, ncol = 2)) %>% dplyr::select(player1 = V1, player2 = V2)
  } else if (match_up == "random" & length(strategies) %% 2 == 0){
    output <- as_data_frame(matrix(sample(strategies, replace=F), ncol = 2)) %>% dplyr::select(player1 = V1, player2 = V2)
  } else {
    stop("Please input valid match_up type 
         or if using 'half_half' or 'random' make sure that your strategies input has an even length")
  }
  
  if (prepare_with_defaults){
    output$params_p1 <- sapply(output$player1, prepare)
    output$params_p2 <- sapply(output$player2, prepare)
  }

  return(output)
}
```

#Functions
```{r functions}
#contains a mixture of functions used for strategies
 
###_________________________ General _________________________###

#utility function
U <- function(a_self, a_op, player, p_matrix){ 
  #INPUT
    #a_self: choice of self
    #a_op: choice of opponnent
    #player: which side of the payof matrix is used. 0: first player, 1: second player
    #returns the point reward
  #OUTPUT
    #The utility of of the given choices for the player.
  
  # get values from payoff matrix for player 1
  a_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # get values from payoff matrix for player 2
  a_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # calculate reward
  (1-player) * #for player 1
    (  a_self * a_op * a_1 +           #if both choose 1 
       a_self * (1 - a_op) * b_1 +     #if you choose 1 and opponent chooses 0
       (1 - a_self) * a_op * c_1 +     #if you choose 0 and the opponent chooses 1
       (1 - a_self) * (1 - a_op) * d_1 #if both choose 0
       ) + 
  player * #for player 2
    (  a_self * a_op * a_2 +           #if both choose 1 
       a_self * (1 - a_op) * b_2 +     #if you choose 1 and opponent chooses 0
       (1 - a_self) * a_op * c_2 +     #if you choose 0 and the opponent chooses 1
       (1 - a_self) * (1 - a_op) * d_2 #if both choose 0
       )
}

#Expected payoff
payoff_difference <- function(p_op_1, player, p_matrix){
  p_op_1*(U(1, 1, player, p_matrix)-U(0, 1, player, p_matrix))+(1-p_op_1)*(U(1, 0, player, p_matrix)-U(0, 0, player, p_matrix))
}

#softmax functions
softmax <- function(e_payoff_diff, beta){ 
  #INPUT
    # expected payoff difference
    # a beta also called bahaivoural temperature - higher make the agent choice more random
  #OUTPUT
    #Returns the probability self choosing 1
  1/(1 + exp(-(e_payoff_diff/beta)))
}



###______________________ ToM specific _______________________###

#Probability of opponnent a_op choosing 1
p_op_1_fun <- function(mu, variance, sigma){
  #INPUT
    # mu is approximate mean of 0-ToM posterior distribution
    # variance is subjective uncertainty of the mean 
    # sigma is the volatility
  #OUTPUT
    #returns probability of opponent choosing 1 
  inv.logit(mu/sqrt(1+(variance+sigma)*3/pi^2)) #example inv.logit(0/sqrt(exp(0)+(1+(-1))*3/pi^2))
}

mu_update <- function(mu, variance, choice_op){
  #INPUT
    # mu from the round before
    # variance form the round before
    # choice_op, which is the opponents choice last round
  #OUTPUT
    # returns an updated mu
  mu + variance * (choice_op - inv.logit(mu))
}

variance_update <- function(mu, variance, sigma) {
  #INPUT
    # mu from last round
    # variance from last round
    # sigma: (volatility) higher values mean lower updating
  #OUTPUT
    # returns an updated variance
  1 / (1 / (variance + sigma) + inv.logit(mu) * (1 - inv.logit(mu)))
}

update_pk <- function(p_k, choice_op, p_op_1){
  #eq. S4 p. ix (9) in Devaine, et al. (2017) appendix
  #calculates P(k)
  p_k <- choice_op* #if opponent choose 1
    ((p_k*p_op_1)/sum(p_k*p_op_1)) + 
    (1-choice_op)* #if opponent choose 0
    (p_k*(1-p_op_1)/sum(p_k*(1-p_op_1))) 
  
  return(p_k)
}

p_op_1_k_fun <- function(mu, variance){
  #INPUT
    #takes a mu and a variance (of opponent)
  #OUTPUT
    #p_op_1_k probability of opponent choosing 1 given k, e.g. P(c_op = 1|k') in loggodds
  
  #this calculation uses a semi-analytical approximation by Daunizeau, J. (2017)
  
  #constants
  a <- 0.205
  b <- -0.319
  c <- 0.781
  d <- 0.870
  
  #calculation
  tmp <- (mu + b * variance^c) / sqrt(1 + a * variance^d)
  p_op_1_k <- log(inv_logit(tmp))
  
  return(p_op_1_k)
}

```

#Simple Agents functions
```{r simple agents}  
###Random Choice with a Bias
RB <- function(ID, player = NULL, p_matrix = NULL, last_round = NULL, params = agents_df$params[agents_df$ID == ID]){
  #RB: Random choice with a bias 
  #INPUT
    #params: a list of 1 element, where the element is the prop of choosing 1 
  #OUTPUT
    #Choice: 0 or 1
  
  #randomly select rabbit or stag (with a slight bias)
  rbinom(1, 1, prob = params[[1]]$prop)
}


###Win-Stay-Loose-Switch
WSLS <- function(ID, player, p_matrix, last_round, 
                 params = agents_df$params[agents_df$ID == ID]){   
  #WSLS: Win stay loose switch
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: noise
  #OUTPUT
    #Choice: 0 or 1
  
  noise <- params$WSLS$noise #get the noise parameter #!# MAKE IT SLOPE PARAMETER INSTEAD, ONE FOR LOOSING AND ONE FOR WINNING
  gain = last_round$points[last_round$ID == ID] #agent's gain last round
  mean_gain = (1 - player) * mean(p_matrix$a1_reward) + player * mean(p_matrix$a2_reward) # above mean gain: win, below mean gain: loss
  max_gain = (1 - player) * max(p_matrix$a1_reward) + player * max(p_matrix$a2_reward) # maximum possible gain
  min_gain = (1 - player) * min(p_matrix$a1_reward) + player * min(p_matrix$a2_reward) # minimum possible gain
  
  #Add the minimum value to all values to avoid negatives. Add 1 to all values to avoid 0's. Has no effect on the outcomes.
  gain = 1 + gain + min_gain
  mean_gain = 1 + mean_gain + min_gain
  max_gain = 1 + max_gain + min_gain
  min_gain = 1 + min_gain + min_gain
  
  if (class(last_round)[1] != "tbl_df") { #initial round or missed trial
    choice <- rbinom(1, 1, prob = 0.5) #make a random choice
  } else {
    if (gain - mean_gain > 0) { #if the player won last round
    
      # Input gain relative to maximum gain in a logistic function. Lower noise param randomizes. When noise->infinite the agent always stays if above mean score.
      probability_stay = 1 / (1 + exp(-noise * (gain/max_gain - mean_gain))) 
      stay = rbinom(1, 1, prob = probability_stay) #whether the agent stays
    
    } else { #if the player lost last round. if gain = mean_gain, probability will be 0.5
      
      #Input gain relative to 
      probability_switch = 1 / (1 + exp(-noise * (min_gain/gain  - mean_gain)))
      stay = rbinom(1, 1, prob = 1 - probability_switch) #whether the agent stays
    } 
    
  choice <-
    last_round$choice[last_round$ID == ID] * stay + #if staying equals last choice
    (1 - last_round$choice[last_round$ID == ID]) * (1 - stay) #if not staying equals opposite of last choice
  }
  
  return(choice)
}

### Softmax Tit for Tat
SoftmaxTitTat <- function(ID, player = NULL, p_matrix, last_round, 
                 params = agents_df$params[agents_df$ID == ID]) {  
  #SoftmaxTitTat: A Tit for tat agent using a softmax function
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: the behaivioural temperature of the agent
  #OUTPUT
    #Choice: 0 or 1
  
  beta <- params$SoftmaxTitTat$bavioral_temp
  c_op <- last_round$choice[last_round$ID != ID]
  
  if (class(last_round)[1] != "tbl_df"){ #initial round or missed trial
    #Start by assuming you opponent will COOP - will make the agents every likely to COOP
    e_payoff_diff <- payoff_difference(1)
    choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  } else {
  e_payoff_diff <- payoff_difference(c_op,  player, p_matrix)
  choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  }
  
  return(choice)
}
```

#Compete function
```{r compete}
compete_all <- function(agent_df, n_rounds = 10, p_matrix, updated_params=T){
  #call the compete function for each pair in the agent_df
  
  #INPUT
    #agent_df: A tibble (dataframe) of prepared agents resulting from create_agents()
    #n_rounds: number of rounds
    #p_matrix: The payoff matrix of the game you want them to be playing
  #OUTPUT
    #The results of the competitions as a tibble
  
  result_list <- apply(agent_df, 1, compete, n_rounds = n_rounds, p_matrix = p_matrix)
  result_df <- result_list %>% bind_rows()
  return(result_df)
}



compete <- function(player1, player2, p1_params, p2_params, n_rounds=10, p_matrix){
  
    #this is for when the compete_all function is used - it simply unpacks the apply function
  if (class(player1) == "list"){
    p2_params <- player1$params_p2
    p1_params <- player1$params_p1
    player2 <- player1$player2
    player1 <- player1$player1

  }
  
    #setting to NA to signify it is the first round
  round_result <- NA
  
    #fetch strategy function
  strat_p1 <- if_else(grepl("-ToM", player1, "k_ToM", player1))
  strat_p2 <- if_else(grepl("-ToM", player2, "k_ToM", player2))
  
  for (round in 1:n_rounds){
      #calls the strategy function for each agent and gives it the ID (a1/a2) as a argument 
    a1_choice <- do.call(strat_p1, 
                         args = list(ID = a1, 
                                     last_round = round_result,
                                     player = 0,
                                     p_matrix = p_matrix
                                     ))
    a2_choice <- do.call(strat_p2, 
                           args = list(ID = a2, 
                                       last_round = round_result,
                                       player = 1,
                                       p_matrix = p_matrix
                                       ))
      
      #generate result
    result <- p_matrix[p_matrix$a1_choice == a1_choice & p_matrix$a2_choice == a2_choice,]
        
    
    #save results    
    round_result <- data_frame(
                               player = c(player1, player2),  
                               choice = c(a1_choice, a2_choice),
                               points = c(result$a1_reward, result$a2_reward), 
                               round_nr = round, 
                               pair = paste(player1, player2, sep = " / "))
  
    if (round == 1){
      result_df <- round_result
    } else {
      result_df <- rbind(result_df, round_result)
    }
  }
  return(result_df)
}



compete <- function(player1_strategy, player2_strategy, player1_params, player2_params, n_rounds = 10, p_matrix){   
  #INPUT
    #n_rounds: number of rounds
    #p_matrix: The payoff matrix of the game you want them to be playing
  #OUTPUT
    #The results of the competition as a tibble
  
  a1 <- pair[1]
  a2 <- pair[2]
  
  round_result <- NA
  
  for (round in 1:n_rounds){
      #calls the strategy function for each agent and gives it the ID (a1/a2) as a argument 
    a1_choice <- do.call(agents_df$strategy_function[agents_df$ID == a1], 
                         args = list(ID = a1, 
                                     last_round = round_result,
                                     player = 0,
                                     p_matrix = p_matrix
                                     ))
    a2_choice <- do.call(agents_df$strategy_function[agents_df$ID == a2], 
                           args = list(ID = a2, 
                                       last_round = round_result,
                                       player = 1,
                                       p_matrix = p_matrix
                                       ))
      
      #generate result
    result <- p_matrix[p_matrix$a1_choice == a1_choice & p_matrix$a2_choice == a2_choice,]
        
      #save results    
    round_result <- data_frame(ID = c(a1, a2),  
                               choice = c(a1_choice, a2_choice),
                               points = c(result$a1_reward, result$a2_reward), 
                               round_nr = round,
                               strategy = c(agents_df$strategy[agents_df$ID == a1],
                                            agents_df$strategy[agents_df$ID == a2]),
                               pair = paste(a1, a2, sep = "/"))
    
    if (round == 1){
      result_df <- round_result
    } else {
      result_df <- rbind(result_df, round_result)
    }
  }
  return(result_df)
}
```


#Running all the things
```{r simulation}  

strategies = c("RB", "WSLS", "SoftmaxTitTat", "0-ToM", "1-ToM", "2-ToM", "3-ToM")

agent_df <- create_agents(strategies, match_up = "RR", prepare_with_defaults = T)
agent_df



```