---
title: "Game Theory Simulations"
author: "K. C. Enevoldsen & P. Waade"
date: "11/24/2018"
output: html_document
editor_options: 
  chunk_output_type: console
---

#Need to do:
#Nice to do:
  Handedness/Bias
  Flag for Noisy - making the gradient prior = 1 for bias
  Perturb Learning
  Dilution/forgetting
  Add output/input/function (e.g. what it does) to all functions
  fiks naming på k eller level (måske k_level?)
  add "(Not an error)" to messages which might be misconstrued as an error message
  add the recursive preperation function to be applied at default if not specified to the k_tom function
  replace '=' with '<-' when saving variables
  test whether the compete function works without hidden states specified
  PLOTS!
  Save i compete function skal være til en list operation
  Add standard deviations to parameter sampling of volatility and temperature - how to limit it to proper values (not negative)

#Done (this is just so we can see some progress)
  change the compete function so it does not input as a double list - should just input as a list (if this is changed RB, WSLS and softmax tittat should also be changed)
  WSLS
  MADE THE GODDAMN SCRIPT RUN!
#Packages and WD
```{r setup, include=FALSE}
# !diagnostics off
#^the above argument prevents the mulitple 'unknown column' warnings (harmless warnings)
knitr::opts_chunk$set(echo = TRUE)
#devtools::install_github("thomasp85/patchwork")
pacman::p_load(plyr, tidyverse, raster, reshape2, knitr, brms, boot, rethinking, groupdata2, patchwork)
```

#Payoff matrices
```{r payoff matrix}  
### Payoff matrices for different games ###

custom_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(0, 0, 0, 0), 
             a2_reward = c(0, 0, 0, 0))

staghunt_matrix <-
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(5, 3, 0, 3), 
             a2_reward = c(5, 0, 3, 3))
 
penny_cooperative_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(1, 0, 0, 1), 
             a2_reward = c(1, 0, 0, 1))

penny_competitive_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(-1, 1, 1, -1), 
             a2_reward = c(1, -1, -1, 1))

prisoner_matrix <- 
  data_frame(a1_choice = c(1, 0, 1, 0), 
             a2_choice = c(1, 1, 0, 0), 
             a1_reward = c(2, 3, -1, 0), 
             a2_reward = c(2, -1, 3, 0))
```

###____________________________________________________________________________________________________________________###
###________________________________________________ DEFINING FUNCTIONS ________________________________________________###
###____________________________________________________________________________________________________________________###

#General functions
```{r general functions}
#Utility function
U <- function(c_self, c_op, player, p_matrix){ 
  #INPUT
    #a_self: choice of self
    #a_op: choice of opponnent
    #player: which side of the payof matrix is used. 0: first player, 1: second player
    #returns the point reward
  #OUTPUT
    #The utility of of the given choices for the player.
  
  # get values from payoff matrix for player 1
  a_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 1 and opponent chooses 0
  c_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 0 and opponent chooses 1
  d_1 <- p_matrix$a1_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # get values from payoff matrix for player 2
  a_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 1] #value if both choose 1
  b_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 1] #value if self chooses 1 and opponent chooses 0
  c_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 1 & p_matrix$a2_choice == 0] #value if self chooses 0 and opponent chooses 1
  d_2 <- p_matrix$a2_reward[p_matrix$a1_choice == 0 & p_matrix$a2_choice == 0] #value if both choose 0
  
  # calculate reward
  reward = 
    (1-player) * #for player 1
      (c_self * c_op * a_1 +           #if both choose 1 
       c_self * (1 - c_op) * b_1 +     #if you choose 1 and opponent chooses 0
       (1 - c_self) * c_op * c_1 +     #if you choose 0 and the opponent chooses 1
       (1 - c_self) * (1 - c_op) * d_1 #if both choose 0
       ) + 
    player * #for player 2
      (c_self * c_op * a_2 +           #if both choose 1 
       c_self * (1 - c_op) * b_2 +     #if you choose 1 and opponent chooses 0
       (1 - c_self) * c_op * c_2 +     #if you choose 0 and the opponent chooses 1
       (1 - c_self) * (1 - c_op) * d_2 #if both choose 0
       )
  
  return(reward)
}

#Expected payoff
expected_payoff_difference <- function(p_op_1, player, p_matrix) {
  
  e_payoff_dif = 
    p_op_1 * (U(1, 1, player, p_matrix) - U(0, 1, player, p_matrix)) +
    (1 - p_op_1) * (U(1, 0, player, p_matrix) - U(0, 0, player, p_matrix))
  
  return(e_payoff_dif)
}


#Softmax function
softmax <- function(e_payoff_diff, behavioural_temperature){
  #Input
  
  #prepare behavioural temperature
  behavioural_temperature = exp(behavioural_temperature)
  
  #Calculate probability of choosing 1
  p_self_1 = 1 / (1 + exp(-(e_payoff_diff / behavioural_temperature)))
  
  return(p_self_1)
}
```

#Simple Agents functions
```{r simple agents}
###Random Choice with a Bias 
RB <- function(params, hidden_states = NULL, player = NULL, p_matrix = NULL, choice_self = NULL, choice_op = NULL, return_hidden_states = F){ 
  #RB: Random choice with a bias 
  #INPUT
    #params: a list of 1 element, where the element is the prop of choosing 1 
  #OUTPUT
    #Choice: 0 or 1
    #Updated parameter: This is for consistency, but is in practice not used
  
  #randomly select rabbit or stag (with a slight bias)
  choice <- rbinom(1, 1, prob = params$prop)
  
  if (return_hidden_states == T){
    return(list(choice = choice, hidden_states = hidden_states))
  } else {
    return(choice)
  }
}

###Win-Stay-Loose-Switch
WSLS <- function(params, hidden_states = NULL, player, p_matrix, choice_self, choice_op, return_hidden_states = F){
  #WSLS: Win-stay-loose-switch
  
  if (is.null(choice_op)) { #initial round or missed trial
        choice = rbinom(1, 1, 0.5) #make random choice
  } else {
  
    #Read in parameters
    stay_prob = params$stay_prob
    switch_prob = params$switch_prob
    #Read in score from last round
    prev_reward = U(choice_self, choice_op, player, p_matrix)
  
    #Calculate the mean of all possible rewards for current player
    mean_reward = (1 - player) * mean(p_matrix$a1_reward) + player * mean(p_matrix$a2_reward)
    
    #Calculate choice
    if (prev_reward > mean_reward) { #if the agent won, i.e. got more than mean amount of points
      stay = rbinom(1, 1, stay_prob) #decide whether agent stays
      choice = stay * choice_self + (1-stay) * (1-choice_self) #staying -> same choice as last
      
    } else if (prev_reward < mean_reward) { #if the agent lost, i.e. got less than mean score
      switch = rbinom(1, 1, switch_prob) #decide whether agent switches
      choice = switch * (1-choice_self) + (1-switch) * choice_self #switching -> opposite choice of last
      
    } else if (prev_reward == mean_reward) { #if the agent got mean score
      choice = rbinom(1, 1, 0.5) #make random choice
    }
  }
  
  if (return_hidden_states == T){
    return(list(choice = choice, hidden_states = hidden_states))
  } else {
    return(choice)
  }
}
  
### Softmax Tit for Tat
SoftmaxTitTat <- function(params, hidden_states = NULL, player, p_matrix, choice_self = NULL, choice_op, return_hidden_states = F){  
  #SoftmaxTitTat: A Tit for tat agent using a softmax function
  #INPUT
    #ID: the ID of the agent
    #round_result: the result of the last round
    #params: a list of 1 element: the behaivioural temperature of the agent
  #OUTPUT
    #Choice: 0 or 1
  
  beta <- params$bavioral_temp
  c_op <- choice_op
  
  if (is.null(c_op)){ #initial round or missed trial
    #Start by assuming you opponent will COOP - will make the agents every likely to COOP
    e_payoff_diff <- expected_payoff_difference(1, player, p_matrix)
    choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  } else {
  e_payoff_diff <- expected_payoff_difference(c_op, player, p_matrix)
  choice <- rbinom(1, 1, prob = softmax(e_payoff_diff, beta))
  }
  
  if (return_hidden_states == T){
    return_hidden_states <- return_hidden_states #does not update params
    return(list(choice = choice, hidden_states = hidden_states))
  } else {
    return(choice)
  }
}
```

#Start k-ToM functions
 
Learning function - 0ToM estimate update
```{r}
basic_variance_update = function(prev_hidden_states, params) {
  #input
  volatility = params$volatility #the volatility parameter reduces learning, assuming that there is noise in the opponents decisions
  prev_variance_basic = prev_hidden_states$own_hidden_states$variance_basic #the uncertainty of opponent probability
  prev_mean_basic = prev_hidden_states$own_hidden_states$mean_basic #the mean estimate of opponent probability
  
  #prepare volatility
  volatility = exp(volatility)
  #prepare variance 
  prev_variance_basic = exp(prev_variance_basic)
  
  #calculate the new variance
  variance_basic = 
    1 / (
      (1 / (volatility + prev_variance_basic)) +
        inv.logit(prev_mean_basic) * (1 - inv.logit(prev_mean_basic)))
  
  #logistic transform
  variance_basic = log(variance_basic)
  
  return(variance_basic)
}

basic_mean_update = function(prev_hidden_states, choices, variance_basic) {
  #input
  prev_c_op = choices[2] #opponent's choice
  prev_mean_basic = prev_hidden_states$own_hidden_states$mean_basic #the uncertainty of opponent probability
  variance_basic #the uncertainty of opponent probability
  
  #prepare variance
  variance = exp(variance_basic)
  
  #calculate the new mean
  mean_basic = prev_mean_basic + variance_basic * (prev_c_op - inv.logit(prev_mean_basic))
  
  return(mean_basic)
}
```

Learning function - update p(k)
```{r}
p_op_1_k_approx_fun <- function(prev_hidden_states, level){
  #this calculation uses a semi-analytical approximation by Daunizeau, J. (2017)
  #this is the VBA_Elogsig
  
  #input
  #for each sophistication level
  prev_mean = prev_hidden_states$own_hidden_states$mean # mean of opponent probability estimation VECTOR
  prev_variance = prev_hidden_states$own_hidden_states$variance # the variance for each estimated parameter MATRIX
  prev_gradient = prev_hidden_states$own_hidden_states$gradient # gradients for each parameter MATRIX
  
  #constants
  a <- 0.205
  b <- -0.319
  c <- 0.781
  d <- 0.870
  
  #prepare variance
  prev_variance_prepared = NULL
  for (level_index in 1:level) {
    prev_variance_prepared[level_index] = t(exp(prev_variance[level,])) %*% prev_gradient[level,]^2 
  }
  
  #calculate estimated probability of opponent choice
  p_op_1_k_approx = 
   inv.logit((prev_mean + b * prev_variance_prepared^c) / sqrt(1 + a * prev_variance_prepared^d))
 
  #log-transform
  p_op_1_k_approx = log(p_op_1_k_approx)
  
  return(p_op_1_k_approx)
}

update_pk <- function(prev_hidden_states, choices, p_op_1_k_approx){
  #input
  prev_c_op = choices[2] #opponent's choice
  #for each sophistication level
  prev_p_k = prev_hidden_states$own_hidden_states$p_k  # probability of sophistication level k VECTOR
  p_op_1_k_approx # probability of opponent choosing 1, approximated semi-analytically VECTOR
  
  #prepare probability
  p_op_1_k_approx = exp(p_op_1_k_approx)
  
  #calculate probability of each possible sophistication level
  p_k =
    prev_c_op* #if opponent chose 1
    ((prev_p_k*p_op_1_k_approx)/sum(prev_p_k*p_op_1_k_approx)) + 
    (1-prev_c_op)* #if opponent chose 0
    (prev_p_k*(1-p_op_1_k_approx)/sum(prev_p_k*(1-p_op_1_k_approx))) 
  
  return(p_k)
}

```

Learning function - 0ToM estimate update
```{r}
basic_variance_update = function(prev_hidden_states, params) {
  #input
  volatility = params$volatility #the volatility parameter reduces learning, assuming that there is noise in the opponents decisions
  prev_variance_basic = prev_hidden_states$own_hidden_states$variance_basic #the uncertainty of opponent probability
  prev_mean_basic = prev_hidden_states$own_hidden_states$mean_basic #the mean estimate of opponent probability
  
  #prepare volatility
  volatility = exp(volatility)
  #prepare variance 
  prev_variance_basic = exp(prev_variance_basic)
  
  #calculate the new variance
  variance_basic = 
    1 / (
      (1 / (volatility + prev_variance_basic)) +
        inv.logit(prev_mean_basic) * (1 - inv.logit(prev_mean_basic)))
  
  #logistic transform
  variance_basic = log(variance_basic)
  
  return(variance_basic)
}

basic_mean_update = function (prev_hidden_states, choices, variance_basic) {
  #input
  prev_c_op = choices[2] #opponent's choice
  prev_mean_basic = prev_hidden_states$own_hidden_states$mean_basic #the uncertainty of opponent probability
  variance_basic #the uncertainty of opponent probability
  
  #prepare variance
  variance_basic = exp(variance_basic)
  
  #calculate the new mean
  mean_basic = prev_mean_basic + variance_basic * (prev_c_op - inv.logit(prev_mean_basic))
  
  return(mean_basic)
}
```

Learning function - update p(k)
```{r}
p_op_1_k_approx_fun <- function(prev_hidden_states, level){
  #this calculation uses a semi-analytical approximation by Daunizeau, J. (2017)
  #this is the VBA_Elogsig
  
  #input
  #for each sophistication level
  prev_mean = prev_hidden_states$own_hidden_states$mean # mean of opponent probability estimation VECTOR
  prev_variance = prev_hidden_states$own_hidden_states$variance # the variance for each estimated parameter MATRIX
  prev_gradient = prev_hidden_states$own_hidden_states$gradient # gradients for each parameter MATRIX
  
  #constants
  a <- 0.205
  b <- -0.319
  c <- 0.781
  d <- 0.870
  
  #prepare variance
  prev_variance_prepared = NULL
  for (level_index in 1:level) {
    prev_variance_prepared[level_index] = t(exp(prev_variance[level,])) %*% prev_gradient[level,]^2 
  }
  
  #calculate estimated probability of opponent choice
  p_op_1_k_approx = 
   inv.logit((prev_mean + b * prev_variance_prepared^c) / sqrt(1 + a * prev_variance_prepared^d))
 
  #log-transform
  p_op_1_k_approx = log(p_op_1_k_approx)
  
  return(p_op_1_k_approx)
}

update_pk <- function(prev_hidden_states, choices, p_op_1_k_approx){
  #input
  prev_c_op = choices[2] #opponent's choice
  #for each sophistication level
  prev_p_k = prev_hidden_states$own_hidden_states$p_k  # probability of sophistication level k VECTOR
  p_op_1_k_approx # probability of opponent choosing 1, approximated semi-analytically VECTOR
  
  #prepare probability
  p_op_1_k_approx = exp(p_op_1_k_approx)
  
  #calculate probability of each possible sophistication level
  p_k =
    prev_c_op* #if opponent chose 1
    ((prev_p_k*p_op_1_k_approx)/sum(prev_p_k*p_op_1_k_approx)) + 
    (1-prev_c_op)* #if opponent chose 0
    (prev_p_k*(1-p_op_1_k_approx)/sum(prev_p_k*(1-p_op_1_k_approx))) 
  
  return(p_k)
}

```

Learning function - updating parameter estimates
```{r}
parameter_variance_update <- function(prev_hidden_states, params, p_k) {
  #input
  volatility = params$volatility
  volatility_dummy = params$volatility_dummy #dummy parable flags which parameters are affected by volatility
  #for each k:
  prev_param_mean = prev_hidden_states$own_hidden_states$param_mean #the mean for each estimated parameter MATRIX
  prev_variance = prev_hidden_states$own_hidden_states$variance #the uncertainty for each estimated parameter MATRIX
  prev_gradient = prev_hidden_states$own_hidden_states$gradient #the gradient for each estimated parameter MATRIX
  p_k #the probability of sopistication level k VECTOR
  
  
  #prepare volatility
  volatility = exp(volatility)*volatility_dummy
  
  #prepare variance
  prev_variance = exp(prev_variance)
  
  #calculate new variance
  variance = 
  1 / 
    (1 / (prev_variance + volatility) + 
       p_k * 
       inv.logit(prev_param_mean) * (1 - inv.logit(prev_param_mean)) * 
       prev_gradient^2)
  
  #logistic transform
  variance = log(variance)
  
  return(variance)
}  


parameter_mean_update <- function(prev_hidden_states, choices, p_k, variance){
  #input
  prev_c_op = choices[2] #opponent's choice
  #for each sophistication level k:
  prev_mean = prev_hidden_states$own_hidden_states$mean #the mean of opponent probability estimation VECTOR
  prev_param_mean = prev_hidden_states$own_hidden_states$param_mean #the mean for each estimated parameter MATRIX
  prev_gradient = prev_hidden_states$own_hidden_states$gradient #the gradient for each estimated parameter MATRIX
  p_k #the probability of sophistication level k VECTOR
  variance #the variance of each estimated parameter MATRIX
  
  #prepare variance
  variance = exp(variance)*prev_gradient
  
  #calculate new mean estimates
  param_mean = 
    prev_param_mean + p_k * variance * (prev_c_op - inv.logit(prev_mean))
  
  #?# "for numerical purposes" - unsure if necessary
  #param_mean = inv.logit(logit(param_mean))
  
  return(param_mean)
}

```

Learning function - updating gradient
```{r}
gradient_update = function(opponent_prev_hidden_states, params, mean, param_mean, reverse_choices, opponent_level, opponent_player, p_matrix.) {
  #input
  opponent_prev_hidden_states #opponent's hidden states, for running the learning function
  reverse_choices #opponent's perspective
  opponent_level #
  opponent_player #
  mean #the mean of opponent probability estimation VECTOR
  param_mean #the mean for each estimated parameter MATRIX
  
  #Make empty list for filling in gradients
  gradient = NULL
  
  for (param in 1:length(param_mean)) {
    
      #calculate increment
      increment = max(abs(1e-4*param_mean[param]), 1e-4)
      
      #use the parameter estimates
      param_mean_incremented = param_mean 
      #but use one of the incremented instead
      param_mean_incremented[param] = param_mean[param] + increment
      
      #Make a list for parameters to be inserted
      opponent_params = list(
      behavioural_temperature = param_mean_incremented[2], #temperature is the second column in the matrix
      volatility = param_mean_incremented[1], #volatility is the first column in the matrix
      volatility_dummy = params$volatility_dummy
      ) 
      
      #run the learning function of opponent using param_mean_temp as parameter values
      opponent_hidden_states_incremented = rec_learning_function(prev_hidden_states = opponent_prev_hidden_states, 
                                                           params = opponent_params, 
                                                           choices = reverse_choices,
                                                           level = opponent_level,
                                                           player = opponent_player,
                                                           p_matrix = p_matrix.) 
      
      #run the decision function of opponent using the temporary hidden states
      mean_incremented = decision_function(hidden_states = opponent_hidden_states_incremented,
                                           params = opponent_params,
                                           player = opponent_player,
                                           level = opponent_level,
                                           p_matrix = p_matrix.)
      
      #calculate the gradient between parameter increment and probability estimate
      gradient[param] = (mean_incremented - mean)/increment
  }
  
  return(gradient)
}
```

Full learning function
```{r} 
rec_learning_function = function(
  prev_hidden_states,
  params,
  choices,
  level,
  player,
  p_matrix
) {
  
  #p_matrix is stored under another name, to avoid recursion-related errors
  p_matrix. = p_matrix
  
  #Make empty list for filling with updated values
  new_hidden_states = list()
  
  if (level == 0) { #If the (simulated) agent is a 0-ToM
    
    #Update 0-ToM's uncertainty of opponent choice probability
    variance_basic = basic_variance_update(prev_hidden_states, params)
    #Update 0-ToM's mean estimate of opponent choice probability
    mean_basic = basic_mean_update(prev_hidden_states, choices, variance_basic)
    
    #Gather own hidden states into one list
    own_hidden_states = list(mean_basic = mean_basic, variance_basic = variance_basic)
    
  } else { #If the (simulated) agent is a K-ToM
  
    #Update p_k
    p_op_1_k_approx <- p_op_1_k_approx_fun(prev_hidden_states, level)
    p_k = update_pk(prev_hidden_states, choices, p_op_1_k_approx)
    
    variance = parameter_variance_update(prev_hidden_states, params, p_k)
    param_mean = parameter_mean_update(prev_hidden_states, choices, p_k, variance)
    
    #Make empty structures for filling in new means
    mean = NULL
    gradient = matrix(NA, ncol = ncol(param_mean), nrow = level) #An empty matrix with a column for each parameter and a row for each level
    
    #Prepare opponent's perspective
    reverse_choices = choices[2:1]
    opponent_player = 1-player
    
    #Now we need to go through each possible opponent's level one at a time. Highest opponent level is 1 lower than own level
    for (level_index in 1:level) {
      
      #Set the simulated opponents level. "level_index" is one higher than the actual level because it isn't possible to index 0
      opponent_level = level_index-1
      #Extract the currently simulated opponent's hidden states
      opponent_hidden_states = prev_hidden_states[[level_index]]
      #Extract the estimated parameters of the current opponent
      opponent_params = list(
        behavioural_temperature = param_mean[level_index, 2], #temperature is the second column in the matrix
        volatility = param_mean[level_index, 1], #volatility is the first column in the matrix
        volatility_dummy = params$volatility_dummy
        ) 
      
      #Simulate opponent learning
      new_opponent_hidden_states = rec_learning_function(prev_hidden_states = opponent_hidden_states,
                                                     params = opponent_params,
                                                     choices = reverse_choices,
                                                     level = opponent_level,
                                                     player = opponent_player,
                                                     p_matrix = p_matrix.) 
      
      #Simulate opponent deciding
      mean[level_index] = decision_function(hidden_states = new_opponent_hidden_states,
                                            params = opponent_params,
                                            player = opponent_player,
                                            level = opponent_level,
                                            p_matrix = p_matrix.)
      
      #Update gradient
      gradient[level_index,] = gradient_update(opponent_prev_hidden_states = opponent_hidden_states,
                                               params = params,
                                               mean = mean[level_index],
                                               param_mean = param_mean[level_index,], #only input the param_mean for the current level
                                               reverse_choices = reverse_choices,
                                               opponent_level = opponent_level,
                                               opponent_player = opponent_player,
                                               p_matrix. = p_matrix.)
      
      #Save opponent's hidden states in the list structure. Name it k-ToM
      eval(parse(text = paste(
        "new_hidden_states$ToM_", 
        opponent_level,
        " = new_opponent_hidden_states",
        sep = "")))
    }
    
    #Gather own hidden states into one list
    own_hidden_states = list(p_k = p_k, mean = mean, param_mean = param_mean, variance = variance, gradient = gradient)
  }
  
  #Save own updated hidden states to new hidden states
  new_hidden_states$own_hidden_states = own_hidden_states
  
  return(new_hidden_states)
}
```

Decision function - 0-ToM opponent probability
```{r}
#Probability of opponnent a_op choosing 1
basic_p_op_1_fun <- function(hidden_states){

  #THIS IS THE VARIANT USED IN THE MATLAB CODE, WHICH DOES NOT USE VOLATILITY
  
  #for each sophistication level k:
  mean_basic = hidden_states$own_hidden_states$mean_basic #mean opponent probability estimate VECTOR
  variance_basic = hidden_states$own_hidden_states$variance_basic #variance of parameter estimates MATRIX
  a = 0.36 #this number is taken from the matlab code in ObsRecGen
  
  #Prepare variance
  variance_basic = exp(variance_basic)
  
  #calculate opponent's probability of choosing 1
  p_op_1_basic = inv.logit(mean_basic / sqrt(1 + a * variance_basic)) 
  
  return(p_op_1_basic)
}

###ALTERNATIVE VERSION 
# basic_p_op_1_fun <- function(own_hidden_states, params){
# 
#   #THIS IS THE VARIANT BASED ON THE EQUATIONS, BUT NOT USED IN THE MATLAB CODE
# 
#   #input
#   volatility = params$volatility
#   mean_basic =  hidden_states$own_hidden_states$mean_basic #mean opponent probability estimate VECTOR
#   variance_basic =  hidden_states$own_hidden_states$variance_basic #variance of parameter estimates MATRIX
# 
#   #Prepare variance
#   variance_basic = exp(variance_basic)
# 
#   #prepare volatility
#   volatility = exp(volatility)
# 
#   #calculate opponent's probability of choosing 1
#   p_op_1_basic = inv.logit(mean_basic/sqrt(1+(variance_basic+volatility)*3/pi^2))
# 
#   return(p_op_1_basic)
# }

```

Decision function - opponent probability
```{r}
#Probability of opponnent a_op choosing 1
p_op_1_k_fun <- function(hidden_states){

  #THIS IS THE VARIANT USED IN THE MATLAB CODE, WHICH DOES NOT USE VOLATILITY
  
  #for each sophistication level k:
  mean = hidden_states$own_hidden_states$mean #mean opponent probability estimate VECTOR
  variance = hidden_states$own_hidden_states$variance #variance of parameter estimates MATRIX
  gradient = hidden_states$own_hidden_states$gradient
  a = 0.36 #this number is taken from the matlab code in ObsRecGen
  
  #Prepare variance
  variance = rowSums(exp(variance) * gradient^2) #summing the variances of each parameter (after weighting by gradient). One sum per sophistication level
  
  #calculate opponent's probability of choosing 1
  p_op_1_k = inv.logit(mean / sqrt(1 + a * variance)) 
  
  return(p_op_1_k)
}

###ALTERNATIVE VERSION
# p_op_1_fun_k <- function(mean, variance, params){
# 
#   #THIS IS THE VARIANT BASED ON THE EQUATIONS, BUT NOT USED IN THE MATLAB CODE
#   
#   #input
#   volatility = params$volatility
#   #for each sophistication level k:
#   mean #mean opponent probability estimate VECTOR
#   variance #variance of parameter estimates MATRIX
#   
#   #prepare volatility
#   dummy = c(1,0,0) #dummy parable flags which parameters are affected by volatility #?# We need to figure out which parameter this is targeting!
#   volatility = exp(volatility)*dummy
#   
#   #Prepare variance
#   variance = sum(exp(variance)*gradient^2)
#   
#   #calculate opponent's probability of choosing 1
#   p_op_1_k = inv.logit(mean/sqrt(1+(variance+volatility)*3/pi^2)) 
#   
#   return(p_op_1_k)
# }

```

Full decision function 
```{r}
decision_function = function(
  hidden_states,
  params,
  player,
  level,
  p_matrix
) { 
  
  if (level == 0) { #If the (simulated) agent is a 0-ToM
    
    #Calculate opponent probability of choosing 1
    p_op_1 = basic_p_op_1_fun(hidden_states)
    
  } else { #If the (simulated) agent is a K-ToM
    
    #Calculate opponent probability of choosing 1, for each k
    p_op_1_k = p_op_1_k_fun(hidden_states)
  
    #extract probabilities for each opponent level
    p_k = hidden_states$own_hidden_states$p_k
    
    #Weigh probabilities by corresponding level probabilities, to calculate an aggregate probability of opponent choosing 1
    p_op_1 = sum(p_op_1_k * p_k)
  }
  
  #Calculate the expected payoff difference
  e_payoff_dif = expected_payoff_difference(p_op_1, player, p_matrix)
  
  #Put into the softmax function to calculate the probability of choosing 1
  p_self_1 = softmax(e_payoff_dif, params$behavioural_temperature)
  
  #Make into logodds
  p_self_1 = logit(p_self_1)
  
  return(p_self_1)
  }

```

Full k-ToM function
```{r}
k_ToM = function(params = "default", hidden_states, player, level = NULL, p_matrix, choice_self, choice_op, return_hidden_states = T) {
  #DESCRIPTION:
  #INPUT:
  #OUTPUT:
  
  if (class(params) != "list"){ #Use default parameter values if nothing else is specified
    message("No parameter values specified, using default values")
    
    params <- list(behavioural_temperature = -1, # these are the values used in the Matlab script
                   volatility = -2,
                   volatility_dummy = c(1,0),  #dummy parable flags which parameters are affected by volatility
                   level = level
                   )
  } 
  

  if (is.null(level)){ #if no level where specified use the one specified in the loop
    level = params$level
  }
  
  #the input comes from last round
  prev_hidden_states = hidden_states
  
  #bind choices together for easy reorganising
  choices = c(choice_self, choice_op)
  
  if (is.null(choice_self)){ #If first round or missed trial
    
    new_hidden_states = prev_hidden_states #No update
    
  } else {
    
    #Update hidden states
     new_hidden_states = 
      rec_learning_function(
        prev_hidden_states,
        params,
        choices,
        level,
        player,
        p_matrix)
  }
  
  #Calculate decision probability
  p_self_1 = 
    decision_function(
      hidden_states = new_hidden_states,
      params,
      player,
      level,
      p_matrix) 
  
  #Make logodds into probability
  p_self_1 = inv.logit(p_self_1)
  
  #Choose
  choice = rbinom(1, 1, p_self_1)
  
  return(list(choice = choice, hidden_states = new_hidden_states))
}
```

Preparing hidden states for the k-ToM function
```{r}
rec_prepare_k_ToM = function(level, priors = "default") {
  
  if (class(priors) != "list"){ #Use default priors if nothing else is specified
    
    message("No priors specified, using default priors")
    
    priors <- list(mean_basic = 0, #agnostic
                   variance_basic = 0, #will be exponated, so exp(0) = 1
                   mean = 0, #agnostic
                   variance = c(0,0), #will be exponated, so exp(0) = 1
                   param_mean = c(0,0), #not agnostic. These will be exponated efore being used as parameters. Szo this is a prior of exp(0) = 1
                   gradient = c(0,0) #is zero. in the Matlab script, one parameter was set to 1 if "flag for noisy" was set
                   )
  }
  
  #Make empty list for filling with updated values
  new_hidden_states = NULL
  
  if (level == 0) { #If the (simulated) agent is a 0-ToM
    
    #Set prior variance on 0-ToM's estimate of opponent choice probability
    variance_basic = priors$variance_basic
    #Set prior mean on 0-ToM's estimate of opponent choice probability
    mean_basic = priors$mean_basic
    
    #Gather own hidden states into one list
    own_hidden_states = list(mean_basic = mean_basic, variance_basic = variance_basic)
    
  } else { #If the (simulated) agent is a K-ToM
    
    #Set priors, one for each of opponent's possible sophistication level
    #Probability is agnostic
    p_k = rep(1, level)/level
    #Mean of opponent choice probability estimate
    mean = rep(priors$mean, level)
    #Variance on parameter estimates
    variance = t(matrix(rep(priors$variance, level),
                        nrow = length(priors$variance)))
    #Mean of parameter estimates
    param_mean = t(matrix(rep(priors$param_mean, level),
                          nrow = length(priors$param_mean)))
    #Gradient
    gradient = t(matrix(rep(priors$gradient, level),
                      nrow = length(priors$gradient)))
    
    #Gather own hidden states into one list
    own_hidden_states = list(p_k = p_k, mean = mean, param_mean = param_mean, variance = variance, gradient = gradient)
      
    #Now we need to go through each possible opponent's level one at a time. Highest opponent level is 1 lower than own level
    for (level_index in 1:level) {
      
      #Set the simulated opponents level. "level_index" is one higher than the actual level because it isn't possible to index 0
      opponent_level = level_index-1
      
      #Get hidden states from simulated opponents
      new_opponent_hidden_states = rec_prepare_k_ToM(level = opponent_level, priors) 
      
      #Save opponent's hidden states in the list structure. Name it k-ToM
      eval(parse(text = paste(
        "new_hidden_states$ToM_",
        opponent_level,
        " = new_opponent_hidden_states",
        sep = "")))
    }
  }
  
  #Save own updated hidden states to new hidden states
  new_hidden_states$own_hidden_states = own_hidden_states
  
  return(new_hidden_states)
}
```

#End k-ToM functions

#Prepare agents functions
```{r prepare}
prepare <- function(strategy_string,
                    RB = c(prop_mean = 0.5,
                           prop_sd = 0.1),
                    WSLS = c(stay_mean = 0.9,
                             stay_sd = 0.1,
                             switch_mean = 0.9,
                             switch_sd = 0.1),
                    TFT= c(temperature_mean = -1,
                           temperature_sd = 0.2),
                    k_ToM = c(volatility_mean = -2,
                              volatility_sd = 0,
                              temperature_mean = -1,
                              temperature_sd = 0),
                    k_ToM_priors = "default"){  
  #INPUT
    #strategy_string: A string of the strategy the agents should apply
    #rb_prop: - #
    #
  #OUTPUT
    #The prepared parameters of the agent.
  
  if (strategy_string == "RB") { #For Random Bias
    params <- list(
      "prop" = inv.logit(rnorm(1, mean = logit(RB[1]), sd = RB[2])))
    hidden_states = "None"
    
  } else if (strategy_string == "WSLS") { #For Win-Stay-Loose-Switch
    params <- list(
      "stay_prob" = inv.logit(rnorm(1, mean = logit(WSLS[1]), sd = WSLS[2])),
      "switch_prob" = inv.logit(rnorm(1, mean = logit(WSLS[3]), sd = WSLS[4])))
    hidden_states = "None"
    
  } else if (strategy_string == "SoftmaxTitTat") { #For Softmax Tit for Tat
    params <- list(
      "bavioral_temp" = log(rnorm(1, mean = exp(TFT[1]), sd = TFT[2])))
    hidden_states = "None"
    
  } else if (grepl("-ToM", strategy_string)) { #if the strategy is a k-ToM
    level <- as.numeric(str_extract(strategy_string, "[0-9]")) #save the level
    params <- list(
      volatility = log(rnorm(1, mean = exp(k_ToM[1]), sd = k_ToM[2])),
      volatility_dummy = c(1,0),
      behavioural_temperature = log(rnorm(1, mean = exp(k_ToM[3]), sd = k_ToM[4])),
      level = level)
    hidden_states <- rec_prepare_k_ToM(level = level, priors = k_ToM_priors)
    
  } else {
    stop("Could not find strategy, try to check for spelling errors")
  }

  result <- list(params = params, hidden_states = hidden_states)
  return(result)
}

prepare_df <- function(agent_df_unprep, args = NULL){ 
  #INPUT
    #agent_df a tibble prepared with prepare_agents
    #args, a character vector of arguments to pass two the prepare function
      #One string should be given pr. agents, an empty string simply used deffaults
      #Note it prepared player1 then Player2 so all player1's should be first in the list
  #OUTPUT
    #The agent_df with prepared features
  
  p1_args <- args[1:(nrow(agent_df_unprep))]
  
  p2_args <- args[-(1:(nrow(agent_df_unprep)))]

    #adding commas for the eval expression, should only be there is there is aditional arguments
  if_else(nchar(p1_args) != 0 ,paste(",", p1_args), p1_args) 
  if_else(nchar(p2_args) != 0 ,paste(",", p2_args), p2_args)
  
    #Make variables for saving - this prevents an error regarding unitialized columns
  agent_df_unprep$params_p1 <- NA
  agent_df_unprep$params_p2 <- NA
  
  agent_df_unprep$hidden_states_p1 <- NA
  agent_df_unprep$hidden_states_p2 <- NA
    
  for (i in 1:length(p1_args)){
    eval(parse(text = 
                 paste("tmp <- prepare(strategy_string = ", "'", agent_df_unprep$player1[i], "'", ", ", p1_args[i],")", sep = "")))
    agent_df_unprep$params_p1[i] <- list(tmp$params)
    agent_df_unprep$hidden_states_p1[i] <- list(tmp$hidden_states)
    eval(parse(text = 
                 paste("tmp <- prepare(strategy_string = ", "'", agent_df_unprep$player2[i], "'", ", ", p2_args[i],")", sep = "")))
    agent_df_unprep$params_p2[i] <- list(tmp$params)
    agent_df_unprep$hidden_states_p2[i] <- list(tmp$hidden_states)
  }
  
  return(agent_df_unprep)
}
```

#Create agents functions
```{r create agents} 
create_agents <- function(strategies, match_up = "RR", prepare_with_defaults = T){
  #strategies: a list of strategies given as a vector
    #implemented agent strategies
        #RB: Random Bias
        #WSLS: Win Stay Loose Switch
        #SoftmaxTitTat: A tit for tat using a softmax function as well as an utility function
        #k-ToM: a k-level theory of mind agent (e.g. k=0 => 0-ToM) 
          #Note that you input the desired k-level in place of k, e.g. 1-ToM
  #match_up:
    #implemented match up types include
      #RR: Round robin, every strategy in the strategies input matched up against every other strategy
      #random: The strategies in the strategies input is randomly matched up
      #half_half: the first half is matched up with the second half
  #prepare_with_defaults:
    #if TRUE prepares the agents using their deffault setup
    #if FALSE does not prepare the agents, in this case we refer to the prepare() function
  #OUTPUT
    #a dataframe with the created agent matched up and prepared if prepare_with_defaults is set to TRUE
  
  if (match_up == "RR"){
    output <- as_data_frame(t(combn(strategies, 2))) %>% dplyr::select(player1 = V1, player2 = V2)
  } else if (match_up == "half_half" & length(strategies) %% 2 == 0){
    output <- as_data_frame(matrix(strategies, ncol = 2)) %>% dplyr::select(player1 = V1, player2 = V2)
  } else if (match_up == "random" & length(strategies) %% 2 == 0){
    output <- as_data_frame(matrix(sample(strategies, replace=F), ncol = 2)) %>% dplyr::select(player1 = V1, player2 = V2)
  } else {
    stop("Please input valid match_up type 
         or if using 'half_half' or 'random' make sure that your strategies input has an even length")
  }
  
  if (prepare_with_defaults){
    
        #calling the prepare function
    tmp1 <- sapply(output$player1, prepare)
    tmp2 <- sapply(output$player2, prepare)
      #save params
    output$params_p1 <- t(tmp1)[,1]
    output$params_p2 <- t(tmp2)[,1]
      #Save hidden states
    output$hidden_states_p1 <- t(tmp1)[,2]
    output$hidden_states_p2 <- t(tmp2)[,2]
  }

  return(output)
}
```

#Compete function
```{r compete}
compete_all <- function(agent_df, p_matrix, n_rounds = 10){  
  #call the compete function for each pair in the agent_df
  
  #INPUT
    #agent_df: A tibble (dataframe) of prepared agents resulting from create_agents()
    #n_rounds: number of rounds
    #p_matrix: The payoff matrix of the game you want them to be playing
  #OUTPUT
    #The results of the competitions as a tibble
  
  result_list <- apply(agent_df, 1, compete, n_rounds = n_rounds, p_matrix = p_matrix)
  result_df <- result_list %>% bind_rows()
  
  return(result_df)
}

compete <- function(player1, player2, p1_params, p2_params, p1_hidden_states = NA, p2_hidden_states = NA, p_matrix, n_rounds = 10){ 
  
    #this is for when the compete_all function is used - it simply unpacks the apply function
  if (class(player1) == "list"){ 
    p2_hidden_states <- list(player1[[6]])
    p1_hidden_states <- list(player1[[5]])
    p2_params <- list(player1[[4]])
    p1_params <- list(player1[[3]])
    player2 <- player1[[2]]
    player1 <- player1[[1]]
  }
  
  if(is.na(p1_hidden_states) | is.na(p2_hidden_states)){ 
    message("One of both of the hidden states were not specified, hopefully your agents don't need them. 
            If they do you should specifiy the hidden states")
  }
  
    #setting to NULL to signify it is the first round
  p1_choice = NULL
  p2_choice = NULL
  
    #fetch strategy function - just the player name unless it is a k-ToM
  strat_p1 <- if_else(grepl("-ToM", player1), "k_ToM", player1)
  strat_p2 <- if_else(grepl("-ToM", player2), "k_ToM", player2)
  
  #Print current pair
  message(paste("Current players: ", player1, " vs ", player2, sep = ""))
  pb <- txtProgressBar(min = 0, max = n_rounds, style = 3)
  
  for (round in 1:n_rounds){
   setTxtProgressBar(pb, round)

    
    
      #calls the strategy function for each agent and gives it the player as a argument 
    p1 <- do.call(strat_p1, 
                         args = list(params = p1_params[[1]],
                                     player = 0,
                                     hidden_states = p1_hidden_states[[1]],
                                     p_matrix = p_matrix,
                                     choice_self = p1_choice,
                                     choice_op = p2_choice,
                                     return_hidden_states = T
                                     ))
  
    p2 <- do.call(strat_p2, 
                           args = list(params = p2_params[[1]],
                                       hidden_states = p2_hidden_states[[1]], 
                                       player = 1,
                                       p_matrix = p_matrix,
                                       choice_self = p2_choice,
                                       choice_op = p1_choice,
                                       return_hidden_states = T
                                       ))
        #params = "default", hidden_states, player, level, p_matrix, choice_self, choice_op, return_hidden_states = T

    
      #results for next round 
    p1_hidden_states <- list(p1$hidden_states)
    p2_hidden_states <- list(p2$hidden_states)
    p1_choice <- p1$choice
    p2_choice <- p2$choice
    
      #generate result
    result <- p_matrix[p_matrix$a1_choice == p1_choice & p_matrix$a2_choice == p2_choice,]
    
    

    
    
    #save results
    round_result <- data_frame(player = c(player1, player2),  
                               choice = c(p1$choice, p2$choice),
                               points = c(result$a1_reward, result$a2_reward), 
                               round_nr = round, 
                               pair = paste(player1, player2, sep = " / "),
                               hidden_states = c(p1_hidden_states, p2_hidden_states)
                               )
  
    if (round == 1){
      result_df <- round_result
    } else {
      result_df <- rbind(result_df, round_result)
    }
  }
  close(pb)
  return(result_df)
}
```


###____________________________________________________________________________________________________________________###
###____________________________________________________ SIMULATION ____________________________________________________###
###____________________________________________________________________________________________________________________###

#Simulation
```{r simulation} 

strategies_unprep = c("RB", "WSLS", "SoftmaxTitTat", "0-ToM", "1-ToM", "2-ToM")


agent_df <- create_agents(strategies_unprep, match_up = "RR", prepare_with_defaults = T)
agent_df

#All competitions in the data_frame
result_df <- compete_all(agent_df, n_rounds = 100, 
        p_matrix = penny_competitive_matrix)

# #A single series competition
# result_df <- compete(player1 = agent_df$player1,
#               player2 = agent_df$player2,
#               p1_params = agent_df$params_p1,
#               p2_params = agent_df$params_p2,
#               p1_hidden_states = agent_df$hidden_states_p1,
#               p2_hidden_states = agent_df$hidden_states_p2,
#               p_matrix = penny_competitive_matrix, #penny_competitive_matrix #prisoner_matrix
#               n_rounds = 100
#               )

#examining results
result_df
result_sum <- result_df %>% group_by(player) %>% summarise(total_point = sum(points), mean_choice = mean(choice))
result_sum

#####plotting#####
hidden_states <- result_df$hidden_states[result_df$player == "2-ToM"]
p_op_1 <- c()
for (i in 1:length(hidden_states)){
  p_op_1 <- c(p_op_1, p_op_1_k_fun(hidden_states[[i]]))
}


plot(seq(length(hidden_states)), #the trials
     agent_df$params_p1[[1]]$prop-p_op_1 #the predictions error
     )

dens(p_op_1)

variance_basic <- c()
for (i in 1:length(hidden_states)){
  variance_basic <- c(variance_basic, result_df$hidden_states[result_df$player == "0-ToM"][[i]]$own_hidden_states$variance_basic)
}

plot(seq(length(hidden_states)), #the trials
     exp(variance_basic)
     )
```

```{r beep}
beepr::beep(4)
```




